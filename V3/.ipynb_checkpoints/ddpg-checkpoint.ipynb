{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">self.action_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "View more on the tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import manipulator\n",
    "\n",
    "# reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "\n",
    "MAX_EP_STEPS = 500\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.002    # learning rate for critic\n",
    "GAMMA = 0.9     # reward discount\n",
    "TAU = 0.01      # soft replacement\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "RENDER = False\n",
    "\n",
    "###############################  DDPG  ####################################\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound,):\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "        self.pointer = 0\n",
    "        self.sess = tf.Session()\n",
    "        self.var = 3.0\n",
    "        # self.a_replace_counter, self.c_replace_counter = 0, 0\n",
    "\n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,\n",
    "        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'r')\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.a = self.build_a_nn(self.S, scope='eval', trainable=True)\n",
    "            a_ = self.build_a_nn(self.S_, scope='target', trainable=False)\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # assign self.a = a in memory when calculating q for td_error,\n",
    "            # otherwise the self.a is from Actor when updating Actor\n",
    "            q = self.build_c_nn(self.S, self.a, scope='eval', trainable=True)\n",
    "            q_ = self.build_c_nn(self.S_, a_, scope='target', trainable=False)\n",
    "\n",
    "        # networks parameters\n",
    "        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        # target net replacement\n",
    "        self.soft_replace = [[tf.assign(at, (1-TAU)*at+TAU*ae), tf.assign(ct, (1-TAU)*ct+TAU*ce)]\n",
    "            for at, ae, ct, ce in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]\n",
    "\n",
    "        q_target = self.R + GAMMA * q_\n",
    "        # in the feed_dic for the td_error, the self.a should change to actions in memory\n",
    "        td_error = tf.losses.mean_squared_error(labels=(self.R + GAMMA * q_), predictions=q)\n",
    "        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, name=\"adam-ink\", var_list = self.ce_params)\n",
    "\n",
    "        a_loss = - tf.reduce_mean(q)    # maximize the q\n",
    "        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.ae_params)\n",
    "\n",
    "        tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "       \n",
    "\n",
    "    def choose_action(self, s):\n",
    "        \n",
    "        mem_mean = np.array([ 5.69844151,  0.63114113,  1.20787513,  0.42712295,  0.0645458 ,\n",
    "        0.01444523, -0.23346819,  5.70642853,  0.6147241 ,  1.21928775,\n",
    "        0.42835155], dtype=np.float32)\n",
    "        \n",
    "        mem_std = np.array([ 7.11894846,  6.20142508,  1.34930158,  0.88119268,  0.87189382,\n",
    "        0.87317526,  0.09533075,  7.12962151,  6.21958685,  1.35649014,\n",
    "        0.88734961], dtype=np.float32)     \n",
    "        \n",
    "        s -= mem_mean[:4]\n",
    "        s /= mem_std[:4]\n",
    "        \n",
    "        a = self.sess.run(self.a, {self.S: s[np.newaxis, :]})[0]\n",
    "        a = np.clip(np.random.normal(a, self.var), -2, 2)\n",
    "        \n",
    "        a *= mem_std[4]\n",
    "        a += mem_mean[4]\n",
    "        # print(\"a: \", a)\n",
    "        return a\n",
    "\n",
    "    def learn(self):\n",
    "        # soft target replacement\n",
    "        self.sess.run(self.soft_replace)\n",
    "\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        bt = self.memory[indices, :]\n",
    "        bs = bt[:, :self.s_dim]\n",
    "        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]\n",
    "        br = bt[:, -self.s_dim - 1: -self.s_dim]\n",
    "        bs_ = bt[:, -self.s_dim:]\n",
    "\n",
    "        self.sess.run(self.atrain, {self.S: bs})\n",
    "        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        # transition = np.hstack((s, a, r, s_))\n",
    "        # index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory\n",
    "        # self.memory[index, :] = transition\n",
    "        # self.pointer += 1\n",
    "\n",
    "        trans = np.hstack((s,a,[r],s_))\n",
    "        \n",
    "        # batch normalization\n",
    "        mem_mean = np.array([ 5.69844151,  0.63114113,  1.20787513,  0.42712295,  0.0645458 ,\n",
    "        0.01444523, -0.23346819,  5.70642853,  0.6147241 ,  1.21928775,\n",
    "        0.42835155], dtype=np.float32)\n",
    "        \n",
    "        mem_std = np.array([ 7.11894846,  6.20142508,  1.34930158,  0.88119268,  0.87189382,\n",
    "        0.87317526,  0.09533075,  7.12962151,  6.21958685,  1.35649014,\n",
    "        0.88734961], dtype=np.float32)\n",
    "        \n",
    "        trans -= mem_mean\n",
    "        trans /= mem_std\n",
    "        \n",
    "        \n",
    "        # print(\"trans: \", trans)\n",
    "        index = self.pointer % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = trans\n",
    "        self.pointer += 1\n",
    "\n",
    "        if self.pointer > MEMORY_CAPACITY:\n",
    "            # print(4557)\n",
    "            self.var *= 0.99995\n",
    "            self.learn()\n",
    "    # def _build_a(self, s, scope, trainable):\n",
    "    #     with tf.variable_scope(scope):\n",
    "    #         net = tf.layers.dense(s, 30, activation=tf.nn.tanh, name='l1', trainable=trainable)\n",
    "    #         a = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, name='a', trainable=trainable)\n",
    "    #         print(\"a: \", a)\n",
    "    #         return tf.multiply(a, self.a_bound, name='scaled_a')\n",
    "    def build_a_nn(self, s, scope, trainable):\n",
    "        # Actor DPG\n",
    "        with tf.variable_scope(scope):\n",
    "            l1 = tf.layers.dense(s, 30, activation = tf.nn.tanh, name = 'l1', trainable = trainable)\n",
    "            a = tf.layers.dense(l1, self.a_dim, activation = tf.nn.tanh, name = 'a', trainable = trainable)     \n",
    "            return tf.multiply(a, self.a_bound, name = \"scaled_a\")  \n",
    "    # def _build_c(self, s, a, scope, trainable):\n",
    "    #     with tf.variable_scope(scope):\n",
    "    #         n_l1 = 30\n",
    "    #         w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable)\n",
    "    #         w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable)\n",
    "    #         b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n",
    "    #         net = tf.nn.tanh(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "    #         return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)\n",
    "    def build_c_nn(self, s, a, scope, trainable):\n",
    "        # Critic Q-leaning\n",
    "        with tf.variable_scope(scope):\n",
    "            n_l1 = 30\n",
    "            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable = trainable)\n",
    "            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable = trainable)\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], trainable = trainable)\n",
    "            net = tf.nn.tanh( tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1 )\n",
    "\n",
    "            q = tf.layers.dense(net, 1, trainable = trainable)\n",
    "            return q\n",
    "\n",
    "    \n",
    "###############################  training  ####################################\n",
    "\n",
    "env = manipulator.manipulator()\n",
    "# env = env.unwrapped\n",
    "# env.seed(1)\n",
    "\n",
    "s_dim = env.state_dim\n",
    "a_dim = env.action_dim\n",
    "a_bound = 0.2\n",
    "\n",
    "ddpg = DDPG(a_dim, s_dim, a_bound)\n",
    "\n",
    "r_save = []\n",
    "\n",
    "# var = 3  # control exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "Episode: 0  Reward: -180 Explore: 3.00\n",
      "i:  1\n",
      "Episode: 1  Reward: -180 Explore: 3.00\n",
      "i:  2\n",
      "Episode: 2  Reward: -183 Explore: 3.00\n",
      "i:  3\n",
      "Episode: 3  Reward: -182 Explore: 3.00\n",
      "i:  4\n",
      "Episode: 4  Reward: -179 Explore: 3.00\n",
      "i:  5\n",
      "Episode: 5  Reward: -184 Explore: 3.00\n",
      "i:  6\n",
      "Episode: 6  Reward: -182 Explore: 3.00\n",
      "i:  7\n",
      "Episode: 7  Reward: -183 Explore: 3.00\n",
      "i:  8\n",
      "Episode: 8  Reward: -181 Explore: 3.00\n",
      "i:  9\n",
      "Episode: 9  Reward: -182 Explore: 3.00\n",
      "i:  10\n",
      "Episode: 10  Reward: -181 Explore: 3.00\n",
      "i:  11\n",
      "Episode: 11  Reward: -182 Explore: 3.00\n",
      "i:  12\n",
      "Episode: 12  Reward: -179 Explore: 3.00\n",
      "i:  13\n",
      "Episode: 13  Reward: -183 Explore: 3.00\n",
      "i:  14\n",
      "Episode: 14  Reward: -181 Explore: 3.00\n",
      "i:  15\n",
      "Episode: 15  Reward: -178 Explore: 3.00\n",
      "i:  16\n",
      "Episode: 16  Reward: -180 Explore: 3.00\n",
      "i:  17\n",
      "Episode: 17  Reward: -182 Explore: 3.00\n",
      "i:  18\n",
      "Episode: 18  Reward: -184 Explore: 3.00\n",
      "i:  19\n",
      "Episode: 19  Reward: -182 Explore: 3.00\n",
      "i:  20\n",
      "Episode: 20  Reward: -182 Explore: 2.93\n",
      "i:  21\n",
      "Episode: 21  Reward: -177 Explore: 2.85\n",
      "i:  22\n",
      "Episode: 22  Reward: -180 Explore: 2.78\n",
      "i:  23\n",
      "Episode: 23  Reward: -179 Explore: 2.71\n",
      "i:  24\n",
      "Episode: 24  Reward: -175 Explore: 2.65\n",
      "i:  25\n",
      "Episode: 25  Reward: -180 Explore: 2.58\n",
      "i:  26\n",
      "Episode: 26  Reward: -176 Explore: 2.52\n",
      "i:  27\n",
      "Episode: 27  Reward: -177 Explore: 2.46\n",
      "i:  28\n",
      "Episode: 28  Reward: -179 Explore: 2.40\n",
      "i:  29\n",
      "Episode: 29  Reward: -179 Explore: 2.34\n",
      "i:  30\n",
      "Episode: 30  Reward: -179 Explore: 2.28\n",
      "i:  31\n",
      "Episode: 31  Reward: -180 Explore: 2.22\n",
      "i:  32\n",
      "Episode: 32  Reward: -177 Explore: 2.17\n",
      "i:  33\n",
      "Episode: 33  Reward: -178 Explore: 2.11\n",
      "i:  34\n",
      "Episode: 34  Reward: -182 Explore: 2.06\n",
      "i:  35\n",
      "Episode: 35  Reward: -180 Explore: 2.01\n",
      "i:  36\n",
      "Episode: 36  Reward: -179 Explore: 1.96\n",
      "i:  37\n",
      "Episode: 37  Reward: -180 Explore: 1.91\n",
      "i:  38\n",
      "Episode: 38  Reward: -179 Explore: 1.87\n",
      "i:  39\n",
      "Episode: 39  Reward: -180 Explore: 1.82\n",
      "i:  40\n",
      "Episode: 40  Reward: -178 Explore: 1.77\n",
      "i:  41\n",
      "Episode: 41  Reward: -177 Explore: 1.73\n",
      "i:  42\n",
      "Episode: 42  Reward: -181 Explore: 1.69\n",
      "i:  43\n",
      "Episode: 43  Reward: -176 Explore: 1.65\n",
      "i:  44\n",
      "Episode: 44  Reward: -179 Explore: 1.61\n",
      "i:  45\n",
      "Episode: 45  Reward: -178 Explore: 1.57\n",
      "i:  46\n",
      "Episode: 46  Reward: -177 Explore: 1.53\n",
      "i:  47\n",
      "Episode: 47  Reward: -176 Explore: 1.49\n",
      "i:  48\n",
      "Episode: 48  Reward: -179 Explore: 1.45\n",
      "i:  49\n",
      "Episode: 49  Reward: -176 Explore: 1.42\n",
      "i:  50\n",
      "Episode: 50  Reward: -179 Explore: 1.38\n",
      "i:  51\n",
      "Episode: 51  Reward: -175 Explore: 1.35\n",
      "i:  52\n",
      "Episode: 52  Reward: -178 Explore: 1.31\n",
      "i:  53\n",
      "Episode: 53  Reward: -175 Explore: 1.28\n",
      "i:  54\n",
      "Episode: 54  Reward: -176 Explore: 1.25\n",
      "i:  55\n",
      "Episode: 55  Reward: -178 Explore: 1.22\n",
      "i:  56\n",
      "Episode: 56  Reward: -175 Explore: 1.19\n",
      "i:  57\n",
      "Episode: 57  Reward: -176 Explore: 1.16\n",
      "i:  58\n",
      "Episode: 58  Reward: -178 Explore: 1.13\n",
      "i:  59\n",
      "Episode: 59  Reward: -173 Explore: 1.10\n",
      "i:  60\n",
      "Episode: 60  Reward: -175 Explore: 1.08\n",
      "i:  61\n",
      "Episode: 61  Reward: -177 Explore: 1.05\n",
      "i:  62\n",
      "Episode: 62  Reward: -173 Explore: 1.02\n",
      "i:  63\n",
      "Episode: 63  Reward: -171 Explore: 1.00\n",
      "i:  64\n",
      "Episode: 64  Reward: -172 Explore: 0.97\n",
      "i:  65\n",
      "Episode: 65  Reward: -172 Explore: 0.95\n",
      "i:  66\n",
      "Episode: 66  Reward: -173 Explore: 0.93\n",
      "i:  67\n",
      "Episode: 67  Reward: -170 Explore: 0.90\n",
      "i:  68\n",
      "Episode: 68  Reward: -174 Explore: 0.88\n",
      "i:  69\n",
      "Episode: 69  Reward: -170 Explore: 0.86\n",
      "i:  70\n",
      "Episode: 70  Reward: -173 Explore: 0.84\n",
      "i:  71\n",
      "Episode: 71  Reward: -176 Explore: 0.82\n",
      "i:  72\n",
      "Episode: 72  Reward: -172 Explore: 0.80\n",
      "i:  73\n",
      "Episode: 73  Reward: -169 Explore: 0.78\n",
      "i:  74\n",
      "Episode: 74  Reward: -168 Explore: 0.76\n",
      "i:  75\n",
      "Episode: 75  Reward: -171 Explore: 0.74\n",
      "i:  76\n",
      "Episode: 76  Reward: -172 Explore: 0.72\n",
      "i:  77\n",
      "Episode: 77  Reward: -173 Explore: 0.70\n",
      "i:  78\n",
      "Episode: 78  Reward: -171 Explore: 0.69\n",
      "i:  79\n",
      "Episode: 79  Reward: -171 Explore: 0.67\n",
      "i:  80\n",
      "Episode: 80  Reward: -171 Explore: 0.65\n",
      "i:  81\n",
      "Episode: 81  Reward: -168 Explore: 0.64\n",
      "i:  82\n",
      "Episode: 82  Reward: -165 Explore: 0.62\n",
      "i:  83\n",
      "Episode: 83  Reward: -167 Explore: 0.61\n",
      "i:  84\n",
      "Episode: 84  Reward: -166 Explore: 0.59\n",
      "i:  85\n",
      "Episode: 85  Reward: -167 Explore: 0.58\n",
      "i:  86\n",
      "Episode: 86  Reward: -166 Explore: 0.56\n",
      "i:  87\n",
      "Episode: 87  Reward: -166 Explore: 0.55\n",
      "i:  88\n",
      "Episode: 88  Reward: -169 Explore: 0.53\n",
      "i:  89\n",
      "Episode: 89  Reward: -165 Explore: 0.52\n",
      "i:  90\n",
      "Episode: 90  Reward: -166 Explore: 0.51\n",
      "i:  91\n",
      "Episode: 91  Reward: -166 Explore: 0.50\n",
      "i:  92\n",
      "Episode: 92  Reward: -165 Explore: 0.48\n",
      "i:  93\n",
      "Episode: 93  Reward: -164 Explore: 0.47\n",
      "i:  94\n",
      "Episode: 94  Reward: -164 Explore: 0.46\n",
      "i:  95\n",
      "Episode: 95  Reward: -163 Explore: 0.45\n",
      "i:  96\n",
      "Episode: 96  Reward: -165 Explore: 0.44\n",
      "i:  97\n",
      "Episode: 97  Reward: -164 Explore: 0.43\n",
      "i:  98\n",
      "Episode: 98  Reward: -166 Explore: 0.42\n",
      "i:  99\n",
      "Episode: 99  Reward: -163 Explore: 0.41\n",
      "i:  100\n",
      "Episode: 100  Reward: -163 Explore: 0.40\n",
      "i:  101\n",
      "Episode: 101  Reward: -162 Explore: 0.39\n",
      "i:  102\n",
      "Episode: 102  Reward: -163 Explore: 0.38\n",
      "i:  103\n",
      "Episode: 103  Reward: -163 Explore: 0.37\n",
      "i:  104\n",
      "Episode: 104  Reward: -163 Explore: 0.36\n",
      "i:  105\n",
      "Episode: 105  Reward: -162 Explore: 0.35\n",
      "i:  106\n",
      "Episode: 106  Reward: -163 Explore: 0.34\n",
      "i:  107\n",
      "Episode: 107  Reward: -162 Explore: 0.33\n",
      "i:  108\n",
      "Episode: 108  Reward: -163 Explore: 0.32\n",
      "i:  109\n",
      "Episode: 109  Reward: -163 Explore: 0.32\n",
      "i:  110\n",
      "Episode: 110  Reward: -163 Explore: 0.31\n",
      "i:  111\n",
      "Episode: 111  Reward: -163 Explore: 0.30\n",
      "i:  112\n",
      "Episode: 112  Reward: -162 Explore: 0.29\n",
      "i:  113\n",
      "Episode: 113  Reward: -163 Explore: 0.29\n",
      "i:  114\n",
      "Episode: 114  Reward: -162 Explore: 0.28\n",
      "i:  115\n",
      "Episode: 115  Reward: -162 Explore: 0.27\n",
      "i:  116\n",
      "Episode: 116  Reward: -162 Explore: 0.27\n",
      "i:  117\n",
      "Episode: 117  Reward: -162 Explore: 0.26\n",
      "i:  118\n",
      "Episode: 118  Reward: -162 Explore: 0.25\n",
      "i:  119\n",
      "Episode: 119  Reward: -162 Explore: 0.25\n",
      "i:  120\n",
      "Episode: 120  Reward: -162 Explore: 0.24\n",
      "i:  121\n",
      "Episode: 121  Reward: -162 Explore: 0.23\n",
      "i:  122\n",
      "Episode: 122  Reward: -162 Explore: 0.23\n",
      "i:  123\n",
      "Episode: 123  Reward: -162 Explore: 0.22\n",
      "i:  124\n",
      "Episode: 124  Reward: -162 Explore: 0.22\n",
      "i:  125\n",
      "Episode: 125  Reward: -162 Explore: 0.21\n",
      "i:  126\n",
      "Episode: 126  Reward: -162 Explore: 0.21\n",
      "i:  127\n",
      "Episode: 127  Reward: -162 Explore: 0.20\n",
      "i:  128\n",
      "Episode: 128  Reward: -161 Explore: 0.20\n",
      "i:  129\n",
      "Episode: 129  Reward: -161 Explore: 0.19\n",
      "i:  130\n",
      "Episode: 130  Reward: -162 Explore: 0.19\n",
      "i:  131\n",
      "Episode: 131  Reward: -161 Explore: 0.18\n",
      "i:  132\n",
      "Episode: 132  Reward: -162 Explore: 0.18\n",
      "i:  133\n",
      "Episode: 133  Reward: -162 Explore: 0.17\n",
      "i:  134\n",
      "Episode: 134  Reward: -161 Explore: 0.17\n",
      "i:  135\n",
      "Episode: 135  Reward: -162 Explore: 0.17\n",
      "i:  136\n",
      "Episode: 136  Reward: -161 Explore: 0.16\n",
      "i:  137\n",
      "Episode: 137  Reward: -162 Explore: 0.16\n",
      "i:  138\n",
      "Episode: 138  Reward: -161 Explore: 0.15\n",
      "i:  139\n",
      "Episode: 139  Reward: -161 Explore: 0.15\n",
      "i:  140\n",
      "Episode: 140  Reward: -161 Explore: 0.15\n",
      "i:  141\n",
      "Episode: 141  Reward: -161 Explore: 0.14\n",
      "i:  142\n",
      "Episode: 142  Reward: -161 Explore: 0.14\n",
      "i:  143\n",
      "Episode: 143  Reward: -161 Explore: 0.14\n",
      "i:  144\n",
      "Episode: 144  Reward: -161 Explore: 0.13\n",
      "i:  145\n",
      "Episode: 145  Reward: -161 Explore: 0.13\n",
      "i:  146\n",
      "Episode: 146  Reward: -162 Explore: 0.13\n",
      "i:  147\n",
      "Episode: 147  Reward: -161 Explore: 0.12\n",
      "i:  148\n",
      "Episode: 148  Reward: -161 Explore: 0.12\n",
      "i:  149\n",
      "Episode: 149  Reward: -161 Explore: 0.12\n",
      "i:  150\n",
      "Episode: 150  Reward: -161 Explore: 0.11\n",
      "i:  151\n",
      "Episode: 151  Reward: -161 Explore: 0.11\n",
      "i:  152\n",
      "Episode: 152  Reward: -161 Explore: 0.11\n",
      "i:  153\n",
      "Episode: 153  Reward: -161 Explore: 0.11\n",
      "i:  154\n",
      "Episode: 154  Reward: -161 Explore: 0.10\n",
      "i:  155\n",
      "Episode: 155  Reward: -161 Explore: 0.10\n",
      "i:  156\n",
      "Episode: 156  Reward: -161 Explore: 0.10\n",
      "i:  157\n",
      "Episode: 157  Reward: -161 Explore: 0.10\n",
      "i:  158\n",
      "Episode: 158  Reward: -161 Explore: 0.09\n",
      "i:  159\n",
      "Episode: 159  Reward: -161 Explore: 0.09\n",
      "i:  160\n",
      "Episode: 160  Reward: -161 Explore: 0.09\n",
      "i:  161\n",
      "Episode: 161  Reward: -161 Explore: 0.09\n",
      "i:  162\n",
      "Episode: 162  Reward: -161 Explore: 0.08\n",
      "i:  163\n",
      "Episode: 163  Reward: -161 Explore: 0.08\n",
      "i:  164\n",
      "Episode: 164  Reward: -161 Explore: 0.08\n",
      "i:  165\n",
      "Episode: 165  Reward: -161 Explore: 0.08\n",
      "i:  166\n",
      "Episode: 166  Reward: -161 Explore: 0.08\n",
      "i:  167\n",
      "Episode: 167  Reward: -161 Explore: 0.07\n",
      "i:  168\n",
      "Episode: 168  Reward: -161 Explore: 0.07\n",
      "i:  169\n",
      "Episode: 169  Reward: -161 Explore: 0.07\n",
      "i:  170\n",
      "Episode: 170  Reward: -161 Explore: 0.07\n",
      "i:  171\n",
      "Episode: 171  Reward: -161 Explore: 0.07\n",
      "i:  172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 172  Reward: -161 Explore: 0.07\n",
      "i:  173\n",
      "Episode: 173  Reward: -161 Explore: 0.06\n",
      "i:  174\n",
      "Episode: 174  Reward: -161 Explore: 0.06\n",
      "i:  175\n",
      "Episode: 175  Reward: -161 Explore: 0.06\n",
      "i:  176\n",
      "Episode: 176  Reward: -161 Explore: 0.06\n",
      "i:  177\n",
      "Episode: 177  Reward: -161 Explore: 0.06\n",
      "i:  178\n",
      "Episode: 178  Reward: -161 Explore: 0.06\n",
      "i:  179\n",
      "Episode: 179  Reward: -161 Explore: 0.05\n",
      "i:  180\n",
      "Episode: 180  Reward: -161 Explore: 0.05\n",
      "i:  181\n",
      "Episode: 181  Reward: -161 Explore: 0.05\n",
      "i:  182\n",
      "Episode: 182  Reward: -161 Explore: 0.05\n",
      "i:  183\n",
      "Episode: 183  Reward: -161 Explore: 0.05\n",
      "i:  184\n",
      "Episode: 184  Reward: -161 Explore: 0.05\n",
      "i:  185\n",
      "Episode: 185  Reward: -161 Explore: 0.05\n",
      "i:  186\n",
      "Episode: 186  Reward: -161 Explore: 0.05\n",
      "i:  187\n",
      "Episode: 187  Reward: -161 Explore: 0.04\n",
      "i:  188\n",
      "Episode: 188  Reward: -161 Explore: 0.04\n",
      "i:  189\n",
      "Episode: 189  Reward: -161 Explore: 0.04\n",
      "i:  190\n",
      "Episode: 190  Reward: -161 Explore: 0.04\n",
      "i:  191\n",
      "Episode: 191  Reward: -161 Explore: 0.04\n",
      "i:  192\n",
      "Episode: 192  Reward: -161 Explore: 0.04\n",
      "i:  193\n",
      "Episode: 193  Reward: -161 Explore: 0.04\n",
      "i:  194\n",
      "Episode: 194  Reward: -161 Explore: 0.04\n",
      "i:  195\n",
      "Episode: 195  Reward: -161 Explore: 0.04\n",
      "i:  196\n",
      "Episode: 196  Reward: -161 Explore: 0.04\n",
      "i:  197\n",
      "Episode: 197  Reward: -161 Explore: 0.04\n",
      "i:  198\n",
      "Episode: 198  Reward: -161 Explore: 0.03\n",
      "i:  199\n",
      "Episode: 199  Reward: -161 Explore: 0.03\n",
      "i:  200\n",
      "Episode: 200  Reward: -161 Explore: 0.03\n",
      "i:  201\n",
      "Episode: 201  Reward: -161 Explore: 0.03\n",
      "i:  202\n",
      "Episode: 202  Reward: -161 Explore: 0.03\n",
      "i:  203\n",
      "Episode: 203  Reward: -161 Explore: 0.03\n",
      "i:  204\n",
      "Episode: 204  Reward: -161 Explore: 0.03\n",
      "i:  205\n",
      "Episode: 205  Reward: -161 Explore: 0.03\n",
      "i:  206\n",
      "Episode: 206  Reward: -161 Explore: 0.03\n",
      "i:  207\n",
      "Episode: 207  Reward: -161 Explore: 0.03\n",
      "i:  208\n",
      "Episode: 208  Reward: -161 Explore: 0.03\n",
      "i:  209\n",
      "Episode: 209  Reward: -161 Explore: 0.03\n",
      "i:  210\n",
      "Episode: 210  Reward: -161 Explore: 0.03\n",
      "i:  211\n",
      "Episode: 211  Reward: -161 Explore: 0.02\n",
      "i:  212\n",
      "Episode: 212  Reward: -161 Explore: 0.02\n",
      "i:  213\n",
      "Episode: 213  Reward: -161 Explore: 0.02\n",
      "i:  214\n",
      "Episode: 214  Reward: -161 Explore: 0.02\n",
      "i:  215\n",
      "Episode: 215  Reward: -161 Explore: 0.02\n",
      "i:  216\n",
      "Episode: 216  Reward: -161 Explore: 0.02\n",
      "i:  217\n",
      "Episode: 217  Reward: -161 Explore: 0.02\n",
      "i:  218\n",
      "Episode: 218  Reward: -161 Explore: 0.02\n",
      "i:  219\n",
      "Episode: 219  Reward: -161 Explore: 0.02\n",
      "i:  220\n",
      "Episode: 220  Reward: -161 Explore: 0.02\n",
      "i:  221\n",
      "Episode: 221  Reward: -161 Explore: 0.02\n",
      "i:  222\n",
      "Episode: 222  Reward: -161 Explore: 0.02\n",
      "i:  223\n",
      "Episode: 223  Reward: -161 Explore: 0.02\n",
      "i:  224\n",
      "Episode: 224  Reward: -161 Explore: 0.02\n",
      "i:  225\n",
      "Episode: 225  Reward: -161 Explore: 0.02\n",
      "i:  226\n",
      "Episode: 226  Reward: -161 Explore: 0.02\n",
      "i:  227\n",
      "Episode: 227  Reward: -161 Explore: 0.02\n",
      "i:  228\n",
      "Episode: 228  Reward: -161 Explore: 0.02\n",
      "i:  229\n",
      "Episode: 229  Reward: -161 Explore: 0.02\n",
      "i:  230\n",
      "Episode: 230  Reward: -161 Explore: 0.02\n",
      "i:  231\n",
      "Episode: 231  Reward: -161 Explore: 0.01\n",
      "i:  232\n",
      "Episode: 232  Reward: -161 Explore: 0.01\n",
      "i:  233\n",
      "Episode: 233  Reward: -161 Explore: 0.01\n",
      "i:  234\n",
      "Episode: 234  Reward: -161 Explore: 0.01\n",
      "i:  235\n",
      "Episode: 235  Reward: -161 Explore: 0.01\n",
      "i:  236\n",
      "Episode: 236  Reward: -161 Explore: 0.01\n",
      "i:  237\n",
      "Episode: 237  Reward: -161 Explore: 0.01\n",
      "i:  238\n",
      "Episode: 238  Reward: -161 Explore: 0.01\n",
      "i:  239\n",
      "Episode: 239  Reward: -161 Explore: 0.01\n",
      "i:  240\n",
      "Episode: 240  Reward: -161 Explore: 0.01\n",
      "i:  241\n",
      "Episode: 241  Reward: -161 Explore: 0.01\n",
      "i:  242\n",
      "Episode: 242  Reward: -161 Explore: 0.01\n",
      "i:  243\n",
      "Episode: 243  Reward: -161 Explore: 0.01\n",
      "i:  244\n",
      "Episode: 244  Reward: -161 Explore: 0.01\n",
      "i:  245\n",
      "Episode: 245  Reward: -161 Explore: 0.01\n",
      "i:  246\n",
      "Episode: 246  Reward: -161 Explore: 0.01\n",
      "i:  247\n",
      "Episode: 247  Reward: -161 Explore: 0.01\n",
      "i:  248\n",
      "Episode: 248  Reward: -161 Explore: 0.01\n",
      "i:  249\n",
      "Episode: 249  Reward: -161 Explore: 0.01\n",
      "i:  250\n",
      "Episode: 250  Reward: -161 Explore: 0.01\n",
      "i:  251\n",
      "Episode: 251  Reward: -161 Explore: 0.01\n",
      "i:  252\n",
      "Episode: 252  Reward: -161 Explore: 0.01\n",
      "i:  253\n",
      "Episode: 253  Reward: -161 Explore: 0.01\n",
      "i:  254\n",
      "Episode: 254  Reward: -161 Explore: 0.01\n",
      "i:  255\n",
      "Episode: 255  Reward: -161 Explore: 0.01\n",
      "i:  256\n",
      "Episode: 256  Reward: -161 Explore: 0.01\n",
      "i:  257\n",
      "Episode: 257  Reward: -161 Explore: 0.01\n",
      "i:  258\n",
      "Episode: 258  Reward: -161 Explore: 0.01\n",
      "i:  259\n",
      "Episode: 259  Reward: -161 Explore: 0.01\n",
      "i:  260\n",
      "Episode: 260  Reward: -161 Explore: 0.01\n",
      "i:  261\n",
      "Episode: 261  Reward: -161 Explore: 0.01\n",
      "i:  262\n",
      "Episode: 262  Reward: -161 Explore: 0.01\n",
      "i:  263\n",
      "Episode: 263  Reward: -161 Explore: 0.01\n",
      "i:  264\n",
      "Episode: 264  Reward: -161 Explore: 0.01\n",
      "i:  265\n",
      "Episode: 265  Reward: -161 Explore: 0.01\n",
      "i:  266\n",
      "Episode: 266  Reward: -161 Explore: 0.01\n",
      "i:  267\n",
      "Episode: 267  Reward: -161 Explore: 0.01\n",
      "i:  268\n",
      "Episode: 268  Reward: -161 Explore: 0.01\n",
      "i:  269\n",
      "Episode: 269  Reward: -161 Explore: 0.01\n",
      "i:  270\n",
      "Episode: 270  Reward: -161 Explore: 0.01\n",
      "i:  271\n",
      "Episode: 271  Reward: -161 Explore: 0.01\n",
      "i:  272\n",
      "Episode: 272  Reward: -161 Explore: 0.01\n",
      "i:  273\n",
      "Episode: 273  Reward: -161 Explore: 0.01\n",
      "i:  274\n",
      "Episode: 274  Reward: -161 Explore: 0.01\n",
      "i:  275\n",
      "Episode: 275  Reward: -161 Explore: 0.00\n",
      "i:  276\n",
      "Episode: 276  Reward: -161 Explore: 0.00\n",
      "i:  277\n",
      "Episode: 277  Reward: -161 Explore: 0.00\n",
      "i:  278\n",
      "Episode: 278  Reward: -161 Explore: 0.00\n",
      "i:  279\n",
      "Episode: 279  Reward: -161 Explore: 0.00\n",
      "i:  280\n",
      "Episode: 280  Reward: -161 Explore: 0.00\n",
      "i:  281\n",
      "Episode: 281  Reward: -161 Explore: 0.00\n",
      "i:  282\n",
      "Episode: 282  Reward: -161 Explore: 0.00\n",
      "i:  283\n",
      "Episode: 283  Reward: -161 Explore: 0.00\n",
      "i:  284\n",
      "Episode: 284  Reward: -161 Explore: 0.00\n",
      "i:  285\n",
      "Episode: 285  Reward: -161 Explore: 0.00\n",
      "i:  286\n",
      "Episode: 286  Reward: -161 Explore: 0.00\n",
      "i:  287\n",
      "Episode: 287  Reward: -161 Explore: 0.00\n",
      "i:  288\n",
      "Episode: 288  Reward: -161 Explore: 0.00\n",
      "i:  289\n",
      "Episode: 289  Reward: -161 Explore: 0.00\n",
      "i:  290\n",
      "Episode: 290  Reward: -161 Explore: 0.00\n",
      "i:  291\n",
      "Episode: 291  Reward: -161 Explore: 0.00\n",
      "i:  292\n",
      "Episode: 292  Reward: -161 Explore: 0.00\n",
      "i:  293\n",
      "Episode: 293  Reward: -161 Explore: 0.00\n",
      "i:  294\n",
      "Episode: 294  Reward: -161 Explore: 0.00\n",
      "i:  295\n",
      "Episode: 295  Reward: -161 Explore: 0.00\n",
      "i:  296\n",
      "Episode: 296  Reward: -161 Explore: 0.00\n",
      "i:  297\n",
      "Episode: 297  Reward: -161 Explore: 0.00\n",
      "i:  298\n",
      "Episode: 298  Reward: -161 Explore: 0.00\n",
      "i:  299\n",
      "Episode: 299  Reward: -161 Explore: 0.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(MAX_EPISODES):\n",
    "    print(\"i: \", i)\n",
    "    s = env.reset()\n",
    "    ep_reward = 0\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "        # Add exploration noise\n",
    "        a = ddpg.choose_action(s)\n",
    "        # a = np.clip(np.random.normal(a, var), -2, 2)    # add randomness to action selection for exploration\n",
    "        # print(\"a: \", a)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        # print(\"r: \", r)\n",
    "        ddpg.store_transition(s, a, r, s_)\n",
    "\n",
    "        # if ddpg.pointer > MEMORY_CAPACITY:\n",
    "        #     ddpg.var *= .9995    # decay the action randomness\n",
    "        #     ddpg.learn()\n",
    "\n",
    "        s = s_\n",
    "        ep_reward += r\n",
    "\n",
    "        \n",
    "\n",
    "        if j == MAX_EP_STEPS-1:\n",
    "            r_save.append(ep_reward)\n",
    "            print('Episode:', i, ' Reward: %i' % int(ep_reward), 'Explore: %.2f' % ddpg.var, )\n",
    "            # if ep_reward > -300:RENDER = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reward = r_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f80ac974400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.plot([1, 2, 3, 4], [10, 20, 25, 30], color='lightblue', linewidth=3) # plot line\n",
    "ax.scatter(range(len(all_reward)), all_reward, color='green', marker='^') # plot points\n",
    "# ax.set_xlim(0.5, 4.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ver-norm-action\n",
    "\n",
    "<img src=\"img/verse-norm-action.png\" >\n",
    "\n",
    "without-verse-norm-action <font color='red'>ERROR</font>\n",
    "\n",
    "<img src=\"img/without-verse-norm-action.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ ddpg.var:  0.09058422353641765 $\n",
    "\n",
    "\n",
    "<img src=\"img/training-curves-400.png\" />\n",
    "\n",
    "$ ddpg.var:  0.03332324041433488 $\n",
    "\n",
    "\n",
    "<img src=\"img/training-curves-500.png\" />\n",
    "\n",
    "$ ddpg.var:  0.012258628581886876 $\n",
    "\n",
    "<img src=\"img/training-curves-600.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about The Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
