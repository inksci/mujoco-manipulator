{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">self.action_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "View more on the tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import manipulator\n",
    "\n",
    "# reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "\n",
    "MAX_EP_STEPS = 500\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.002    # learning rate for critic\n",
    "GAMMA = 0.9     # reward discount\n",
    "TAU = 0.01      # soft replacement\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "RENDER = False\n",
    "\n",
    "###############################  DDPG  ####################################\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound,):\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "        self.pointer = 0\n",
    "        self.sess = tf.Session()\n",
    "        self.var = 3.0\n",
    "        # self.a_replace_counter, self.c_replace_counter = 0, 0\n",
    "\n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,\n",
    "        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'r')\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.a = self.build_a_nn(self.S, scope='eval', trainable=True)\n",
    "            a_ = self.build_a_nn(self.S_, scope='target', trainable=False)\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # assign self.a = a in memory when calculating q for td_error,\n",
    "            # otherwise the self.a is from Actor when updating Actor\n",
    "            q = self.build_c_nn(self.S, self.a, scope='eval', trainable=True)\n",
    "            q_ = self.build_c_nn(self.S_, a_, scope='target', trainable=False)\n",
    "\n",
    "        # networks parameters\n",
    "        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        # target net replacement\n",
    "        self.soft_replace = [[tf.assign(at, (1-TAU)*at+TAU*ae), tf.assign(ct, (1-TAU)*ct+TAU*ce)]\n",
    "            for at, ae, ct, ce in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]\n",
    "\n",
    "        q_target = self.R + GAMMA * q_\n",
    "        # in the feed_dic for the td_error, the self.a should change to actions in memory\n",
    "        td_error = tf.losses.mean_squared_error(labels=(self.R + GAMMA * q_), predictions=q)\n",
    "        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, name=\"adam-ink\", var_list = self.ce_params)\n",
    "\n",
    "        a_loss = - tf.reduce_mean(q)    # maximize the q\n",
    "        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.ae_params)\n",
    "\n",
    "        tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "       \n",
    "\n",
    "    def choose_action(self, s):\n",
    "        \n",
    "        mem_mean = np.array([ 0.48621982, -0.39433715,  0.55701429,  0.71694756,  0.93387568,\n",
    "        0.37078592, -0.14167269,  0.40072495,  0.25386503,  0.4805119 ,\n",
    "        0.09596774, -0.11102104,  0.16225703,  0.06218311,  0.19568023,\n",
    "       -0.26812184,  0.48575708, -0.39573932,  0.55680048,  0.71941608,\n",
    "        0.93555212,  0.37175739, -0.14246416,  0.40183851,  0.25530386,\n",
    "        0.48238301], dtype=np.float32)\n",
    "        \n",
    "        mem_std = np.array([ 0.77474898,  0.72022182,  0.56043774,  0.90410036,  0.12644294,\n",
    "        0.24395365,  0.3367165 ,  0.20749338,  0.49386111,  0.27530244,\n",
    "        0.13662679,  0.10744574,  0.08928268,  0.18348816,  0.02752163,\n",
    "        0.06287373,  0.77596879,  0.72043914,  0.56105018,  0.90423542,\n",
    "        0.11954755,  0.24412741,  0.3376236 ,  0.20702067,  0.49511331,\n",
    "        0.27521592], dtype=np.float32)    \n",
    "    \n",
    "    \n",
    "        s -= mem_mean[:self.s_dim]\n",
    "        s /= mem_std[:self.s_dim]\n",
    "        \n",
    "        a = self.sess.run(self.a, {self.S: s[np.newaxis, :]})[0]\n",
    "        a = np.clip(np.random.normal(a, self.var), -2, 2)\n",
    "        \n",
    "        a *= mem_std[self.s_dim: self.s_dim + self.a_dim]\n",
    "        a += mem_mean[self.s_dim: self.s_dim + self.a_dim]\n",
    "\n",
    "        return a\n",
    "\n",
    "    def learn(self):\n",
    "        # soft target replacement\n",
    "        self.sess.run(self.soft_replace)\n",
    "\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        bt = self.memory[indices, :]\n",
    "        bs = bt[:, :self.s_dim]\n",
    "        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]\n",
    "        br = bt[:, -self.s_dim - 1: -self.s_dim]\n",
    "        bs_ = bt[:, -self.s_dim:]\n",
    "\n",
    "        self.sess.run(self.atrain, {self.S: bs})\n",
    "        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "\n",
    "        trans = np.hstack((s,a,[r],s_))\n",
    "        \n",
    "        # batch normalization\n",
    "        mem_mean = np.array([ 0.48621982, -0.39433715,  0.55701429,  0.71694756,  0.93387568,\n",
    "        0.37078592, -0.14167269,  0.40072495,  0.25386503,  0.4805119 ,\n",
    "        0.09596774, -0.11102104,  0.16225703,  0.06218311,  0.19568023,\n",
    "       -0.26812184,  0.48575708, -0.39573932,  0.55680048,  0.71941608,\n",
    "        0.93555212,  0.37175739, -0.14246416,  0.40183851,  0.25530386,\n",
    "        0.48238301], dtype=np.float32)\n",
    "        \n",
    "        mem_std = np.array([ 0.77474898,  0.72022182,  0.56043774,  0.90410036,  0.12644294,\n",
    "        0.24395365,  0.3367165 ,  0.20749338,  0.49386111,  0.27530244,\n",
    "        0.13662679,  0.10744574,  0.08928268,  0.18348816,  0.02752163,\n",
    "        0.06287373,  0.77596879,  0.72043914,  0.56105018,  0.90423542,\n",
    "        0.11954755,  0.24412741,  0.3376236 ,  0.20702067,  0.49511331,\n",
    "        0.27521592], dtype=np.float32)    \n",
    "        \n",
    "        trans -= mem_mean\n",
    "        trans /= mem_std\n",
    "        \n",
    "        \n",
    "        # print(\"trans: \", trans)\n",
    "        index = self.pointer % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = trans\n",
    "        self.pointer += 1\n",
    "\n",
    "        if self.pointer > MEMORY_CAPACITY:\n",
    "            self.var *= 0.99995\n",
    "            self.learn()\n",
    "    def build_a_nn(self, s, scope, trainable):\n",
    "        # Actor DPG\n",
    "        with tf.variable_scope(scope):\n",
    "            l1 = tf.layers.dense(s, 30, activation = tf.nn.tanh, name = 'l1', trainable = trainable)\n",
    "            a = tf.layers.dense(l1, self.a_dim, activation = tf.nn.tanh, name = 'a', trainable = trainable)     \n",
    "            return tf.multiply(a, self.a_bound, name = \"scaled_a\")  \n",
    "    # def _build_c(self, s, a, scope, trainable):\n",
    "    #     with tf.variable_scope(scope):\n",
    "    #         n_l1 = 30\n",
    "    #         w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable)\n",
    "    #         w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable)\n",
    "    #         b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n",
    "    #         net = tf.nn.tanh(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "    #         return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)\n",
    "    def build_c_nn(self, s, a, scope, trainable):\n",
    "        # Critic Q-leaning\n",
    "        with tf.variable_scope(scope):\n",
    "            n_l1 = 30\n",
    "            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable = trainable)\n",
    "            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable = trainable)\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], trainable = trainable)\n",
    "            net = tf.nn.tanh( tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1 )\n",
    "\n",
    "            q = tf.layers.dense(net, 1, trainable = trainable)\n",
    "            return q\n",
    "\n",
    "    \n",
    "###############################  training  ####################################\n",
    "\n",
    "env = manipulator.manipulator()\n",
    "# env = env.unwrapped\n",
    "# env.seed(1)\n",
    "\n",
    "s_dim = env.state_dim\n",
    "a_dim = env.action_dim\n",
    "a_bound = 0.2\n",
    "\n",
    "ddpg = DDPG(a_dim, s_dim, a_bound)\n",
    "\n",
    "r_save = []\n",
    "\n",
    "# var = 3  # control exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_save = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "Episode: 0  Reward: -126 Explore: 3.00\n",
      "i:  1\n",
      "Episode: 1  Reward: -126 Explore: 3.00\n",
      "i:  2\n",
      "Episode: 2  Reward: -128 Explore: 3.00\n",
      "i:  3\n",
      "Episode: 3  Reward: -129 Explore: 3.00\n",
      "i:  4\n",
      "Episode: 4  Reward: -128 Explore: 3.00\n",
      "i:  5\n",
      "Episode: 5  Reward: -128 Explore: 3.00\n",
      "i:  6\n",
      "Episode: 6  Reward: -127 Explore: 3.00\n",
      "i:  7\n",
      "Episode: 7  Reward: -126 Explore: 3.00\n",
      "i:  8\n",
      "Episode: 8  Reward: -129 Explore: 3.00\n",
      "i:  9\n",
      "Episode: 9  Reward: -126 Explore: 3.00\n",
      "i:  10\n",
      "Episode: 10  Reward: -126 Explore: 3.00\n",
      "i:  11\n",
      "Episode: 11  Reward: -132 Explore: 3.00\n",
      "i:  12\n",
      "Episode: 12  Reward: -127 Explore: 3.00\n",
      "i:  13\n",
      "Episode: 13  Reward: -126 Explore: 3.00\n",
      "i:  14\n",
      "Episode: 14  Reward: -128 Explore: 3.00\n",
      "i:  15\n",
      "Episode: 15  Reward: -129 Explore: 3.00\n",
      "i:  16\n",
      "Episode: 16  Reward: -131 Explore: 3.00\n",
      "i:  17\n",
      "Episode: 17  Reward: -127 Explore: 3.00\n",
      "i:  18\n",
      "Episode: 18  Reward: -128 Explore: 3.00\n",
      "i:  19\n",
      "Episode: 19  Reward: -129 Explore: 3.00\n",
      "i:  20\n",
      "Episode: 20  Reward: -129 Explore: 2.93\n",
      "i:  21\n",
      "Episode: 21  Reward: -132 Explore: 2.85\n",
      "i:  22\n",
      "Episode: 22  Reward: -131 Explore: 2.78\n",
      "i:  23\n",
      "Episode: 23  Reward: -135 Explore: 2.71\n",
      "i:  24\n",
      "Episode: 24  Reward: -133 Explore: 2.65\n",
      "i:  25\n",
      "Episode: 25  Reward: -129 Explore: 2.58\n",
      "i:  26\n",
      "Episode: 26  Reward: -133 Explore: 2.52\n",
      "i:  27\n",
      "Episode: 27  Reward: -126 Explore: 2.46\n",
      "i:  28\n",
      "Episode: 28  Reward: -131 Explore: 2.40\n",
      "i:  29\n",
      "Episode: 29  Reward: -125 Explore: 2.34\n",
      "i:  30\n",
      "Episode: 30  Reward: -130 Explore: 2.28\n",
      "i:  31\n",
      "Episode: 31  Reward: -134 Explore: 2.22\n",
      "i:  32\n",
      "Episode: 32  Reward: -131 Explore: 2.17\n",
      "i:  33\n",
      "Episode: 33  Reward: -125 Explore: 2.11\n",
      "i:  34\n",
      "Episode: 34  Reward: -127 Explore: 2.06\n",
      "i:  35\n",
      "Episode: 35  Reward: -130 Explore: 2.01\n",
      "i:  36\n",
      "Episode: 36  Reward: -127 Explore: 1.96\n",
      "i:  37\n",
      "Episode: 37  Reward: -127 Explore: 1.91\n",
      "i:  38\n",
      "Episode: 38  Reward: -131 Explore: 1.87\n",
      "i:  39\n",
      "Episode: 39  Reward: -130 Explore: 1.82\n",
      "i:  40\n",
      "Episode: 40  Reward: -132 Explore: 1.77\n",
      "i:  41\n",
      "Episode: 41  Reward: -128 Explore: 1.73\n",
      "i:  42\n",
      "Episode: 42  Reward: -128 Explore: 1.69\n",
      "i:  43\n",
      "Episode: 43  Reward: -134 Explore: 1.65\n",
      "i:  44\n",
      "Episode: 44  Reward: -128 Explore: 1.61\n",
      "i:  45\n",
      "Episode: 45  Reward: -127 Explore: 1.57\n",
      "i:  46\n",
      "Episode: 46  Reward: -129 Explore: 1.53\n",
      "i:  47\n",
      "Episode: 47  Reward: -123 Explore: 1.49\n",
      "i:  48\n",
      "Episode: 48  Reward: -123 Explore: 1.45\n",
      "i:  49\n",
      "Episode: 49  Reward: -125 Explore: 1.42\n",
      "i:  50\n",
      "Episode: 50  Reward: -132 Explore: 1.38\n",
      "i:  51\n",
      "Episode: 51  Reward: -128 Explore: 1.35\n",
      "i:  52\n",
      "Episode: 52  Reward: -127 Explore: 1.31\n",
      "i:  53\n",
      "Episode: 53  Reward: -125 Explore: 1.28\n",
      "i:  54\n",
      "Episode: 54  Reward: -123 Explore: 1.25\n",
      "i:  55\n",
      "Episode: 55  Reward: -126 Explore: 1.22\n",
      "i:  56\n",
      "Episode: 56  Reward: -126 Explore: 1.19\n",
      "i:  57\n",
      "Episode: 57  Reward: -128 Explore: 1.16\n",
      "i:  58\n",
      "Episode: 58  Reward: -129 Explore: 1.13\n",
      "i:  59\n",
      "Episode: 59  Reward: -127 Explore: 1.10\n",
      "i:  60\n",
      "Episode: 60  Reward: -124 Explore: 1.08\n",
      "i:  61\n",
      "Episode: 61  Reward: -123 Explore: 1.05\n",
      "i:  62\n",
      "Episode: 62  Reward: -123 Explore: 1.02\n",
      "i:  63\n",
      "Episode: 63  Reward: -124 Explore: 1.00\n",
      "i:  64\n",
      "Episode: 64  Reward: -126 Explore: 0.97\n",
      "i:  65\n",
      "Episode: 65  Reward: -123 Explore: 0.95\n",
      "i:  66\n",
      "Episode: 66  Reward: -127 Explore: 0.93\n",
      "i:  67\n",
      "Episode: 67  Reward: -126 Explore: 0.90\n",
      "i:  68\n",
      "Episode: 68  Reward: -122 Explore: 0.88\n",
      "i:  69\n",
      "Episode: 69  Reward: -120 Explore: 0.86\n",
      "i:  70\n",
      "Episode: 70  Reward: -120 Explore: 0.84\n",
      "i:  71\n",
      "Episode: 71  Reward: -120 Explore: 0.82\n",
      "i:  72\n",
      "Episode: 72  Reward: -120 Explore: 0.80\n",
      "i:  73\n",
      "Episode: 73  Reward: -122 Explore: 0.78\n",
      "i:  74\n",
      "Episode: 74  Reward: -121 Explore: 0.76\n",
      "i:  75\n",
      "Episode: 75  Reward: -123 Explore: 0.74\n",
      "i:  76\n",
      "Episode: 76  Reward: -122 Explore: 0.72\n",
      "i:  77\n",
      "Episode: 77  Reward: -121 Explore: 0.70\n",
      "i:  78\n",
      "Episode: 78  Reward: -119 Explore: 0.69\n",
      "i:  79\n",
      "Episode: 79  Reward: -120 Explore: 0.67\n",
      "i:  80\n",
      "Episode: 80  Reward: -120 Explore: 0.65\n",
      "i:  81\n",
      "Episode: 81  Reward: -121 Explore: 0.64\n",
      "i:  82\n",
      "Episode: 82  Reward: -121 Explore: 0.62\n",
      "i:  83\n",
      "Episode: 83  Reward: -119 Explore: 0.61\n",
      "i:  84\n",
      "Episode: 84  Reward: -122 Explore: 0.59\n",
      "i:  85\n",
      "Episode: 85  Reward: -121 Explore: 0.58\n",
      "i:  86\n",
      "Episode: 86  Reward: -120 Explore: 0.56\n",
      "i:  87\n",
      "Episode: 87  Reward: -122 Explore: 0.55\n",
      "i:  88\n",
      "Episode: 88  Reward: -128 Explore: 0.53\n",
      "i:  89\n",
      "Episode: 89  Reward: -119 Explore: 0.52\n",
      "i:  90\n",
      "Episode: 90  Reward: -120 Explore: 0.51\n",
      "i:  91\n",
      "Episode: 91  Reward: -120 Explore: 0.50\n",
      "i:  92\n",
      "Episode: 92  Reward: -120 Explore: 0.48\n",
      "i:  93\n",
      "Episode: 93  Reward: -122 Explore: 0.47\n",
      "i:  94\n",
      "Episode: 94  Reward: -120 Explore: 0.46\n",
      "i:  95\n",
      "Episode: 95  Reward: -120 Explore: 0.45\n",
      "i:  96\n",
      "Episode: 96  Reward: -121 Explore: 0.44\n",
      "i:  97\n",
      "Episode: 97  Reward: -120 Explore: 0.43\n",
      "i:  98\n",
      "Episode: 98  Reward: -120 Explore: 0.42\n",
      "i:  99\n",
      "Episode: 99  Reward: -119 Explore: 0.41\n",
      "i:  100\n",
      "Episode: 100  Reward: -120 Explore: 0.40\n",
      "i:  101\n",
      "Episode: 101  Reward: -122 Explore: 0.39\n",
      "i:  102\n",
      "Episode: 102  Reward: -120 Explore: 0.38\n",
      "i:  103\n",
      "Episode: 103  Reward: -119 Explore: 0.37\n",
      "i:  104\n",
      "Episode: 104  Reward: -120 Explore: 0.36\n",
      "i:  105\n",
      "Episode: 105  Reward: -120 Explore: 0.35\n",
      "i:  106\n",
      "Episode: 106  Reward: -120 Explore: 0.34\n",
      "i:  107\n",
      "Episode: 107  Reward: -119 Explore: 0.33\n",
      "i:  108\n",
      "Episode: 108  Reward: -119 Explore: 0.32\n",
      "i:  109\n",
      "Episode: 109  Reward: -119 Explore: 0.32\n",
      "i:  110\n",
      "Episode: 110  Reward: -120 Explore: 0.31\n",
      "i:  111\n",
      "Episode: 111  Reward: -119 Explore: 0.30\n",
      "i:  112\n",
      "Episode: 112  Reward: -120 Explore: 0.29\n",
      "i:  113\n",
      "Episode: 113  Reward: -119 Explore: 0.29\n",
      "i:  114\n",
      "Episode: 114  Reward: -121 Explore: 0.28\n",
      "i:  115\n",
      "Episode: 115  Reward: -120 Explore: 0.27\n",
      "i:  116\n",
      "Episode: 116  Reward: -119 Explore: 0.27\n",
      "i:  117\n",
      "Episode: 117  Reward: -119 Explore: 0.26\n",
      "i:  118\n",
      "Episode: 118  Reward: -119 Explore: 0.25\n",
      "i:  119\n",
      "Episode: 119  Reward: -120 Explore: 0.25\n",
      "i:  120\n",
      "Episode: 120  Reward: -119 Explore: 0.24\n",
      "i:  121\n",
      "Episode: 121  Reward: -120 Explore: 0.23\n",
      "i:  122\n",
      "Episode: 122  Reward: -120 Explore: 0.23\n",
      "i:  123\n",
      "Episode: 123  Reward: -119 Explore: 0.22\n",
      "i:  124\n",
      "Episode: 124  Reward: -120 Explore: 0.22\n",
      "i:  125\n",
      "Episode: 125  Reward: -119 Explore: 0.21\n",
      "i:  126\n",
      "Episode: 126  Reward: -120 Explore: 0.21\n",
      "i:  127\n",
      "Episode: 127  Reward: -119 Explore: 0.20\n",
      "i:  128\n",
      "Episode: 128  Reward: -120 Explore: 0.20\n",
      "i:  129\n",
      "Episode: 129  Reward: -120 Explore: 0.19\n",
      "i:  130\n",
      "Episode: 130  Reward: -119 Explore: 0.19\n",
      "i:  131\n",
      "Episode: 131  Reward: -119 Explore: 0.18\n",
      "i:  132\n",
      "Episode: 132  Reward: -119 Explore: 0.18\n",
      "i:  133\n",
      "Episode: 133  Reward: -119 Explore: 0.17\n",
      "i:  134\n",
      "Episode: 134  Reward: -119 Explore: 0.17\n",
      "i:  135\n",
      "Episode: 135  Reward: -119 Explore: 0.17\n",
      "i:  136\n",
      "Episode: 136  Reward: -119 Explore: 0.16\n",
      "i:  137\n",
      "Episode: 137  Reward: -119 Explore: 0.16\n",
      "i:  138\n",
      "Episode: 138  Reward: -119 Explore: 0.15\n",
      "i:  139\n",
      "Episode: 139  Reward: -119 Explore: 0.15\n",
      "i:  140\n",
      "Episode: 140  Reward: -119 Explore: 0.15\n",
      "i:  141\n",
      "Episode: 141  Reward: -119 Explore: 0.14\n",
      "i:  142\n",
      "Episode: 142  Reward: -119 Explore: 0.14\n",
      "i:  143\n",
      "Episode: 143  Reward: -119 Explore: 0.14\n",
      "i:  144\n",
      "Episode: 144  Reward: -119 Explore: 0.13\n",
      "i:  145\n",
      "Episode: 145  Reward: -119 Explore: 0.13\n",
      "i:  146\n",
      "Episode: 146  Reward: -119 Explore: 0.13\n",
      "i:  147\n",
      "Episode: 147  Reward: -119 Explore: 0.12\n",
      "i:  148\n",
      "Episode: 148  Reward: -119 Explore: 0.12\n",
      "i:  149\n",
      "Episode: 149  Reward: -119 Explore: 0.12\n",
      "i:  150\n",
      "Episode: 150  Reward: -119 Explore: 0.11\n",
      "i:  151\n",
      "Episode: 151  Reward: -119 Explore: 0.11\n",
      "i:  152\n",
      "Episode: 152  Reward: -119 Explore: 0.11\n",
      "i:  153\n",
      "Episode: 153  Reward: -119 Explore: 0.11\n",
      "i:  154\n",
      "Episode: 154  Reward: -119 Explore: 0.10\n",
      "i:  155\n",
      "Episode: 155  Reward: -119 Explore: 0.10\n",
      "i:  156\n",
      "Episode: 156  Reward: -119 Explore: 0.10\n",
      "i:  157\n",
      "Episode: 157  Reward: -119 Explore: 0.10\n",
      "i:  158\n",
      "Episode: 158  Reward: -119 Explore: 0.09\n",
      "i:  159\n",
      "Episode: 159  Reward: -119 Explore: 0.09\n",
      "i:  160\n",
      "Episode: 160  Reward: -119 Explore: 0.09\n",
      "i:  161\n",
      "Episode: 161  Reward: -119 Explore: 0.09\n",
      "i:  162\n",
      "Episode: 162  Reward: -119 Explore: 0.08\n",
      "i:  163\n",
      "Episode: 163  Reward: -119 Explore: 0.08\n",
      "i:  164\n",
      "Episode: 164  Reward: -119 Explore: 0.08\n",
      "i:  165\n",
      "Episode: 165  Reward: -119 Explore: 0.08\n",
      "i:  166\n",
      "Episode: 166  Reward: -119 Explore: 0.08\n",
      "i:  167\n",
      "Episode: 167  Reward: -119 Explore: 0.07\n",
      "i:  168\n",
      "Episode: 168  Reward: -119 Explore: 0.07\n",
      "i:  169\n",
      "Episode: 169  Reward: -119 Explore: 0.07\n",
      "i:  170\n",
      "Episode: 170  Reward: -119 Explore: 0.07\n",
      "i:  171\n",
      "Episode: 171  Reward: -119 Explore: 0.07\n",
      "i:  172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 172  Reward: -119 Explore: 0.07\n",
      "i:  173\n",
      "Episode: 173  Reward: -119 Explore: 0.06\n",
      "i:  174\n",
      "Episode: 174  Reward: -119 Explore: 0.06\n",
      "i:  175\n",
      "Episode: 175  Reward: -119 Explore: 0.06\n",
      "i:  176\n",
      "Episode: 176  Reward: -119 Explore: 0.06\n",
      "i:  177\n",
      "Episode: 177  Reward: -119 Explore: 0.06\n",
      "i:  178\n",
      "Episode: 178  Reward: -119 Explore: 0.06\n",
      "i:  179\n",
      "Episode: 179  Reward: -119 Explore: 0.05\n",
      "i:  180\n",
      "Episode: 180  Reward: -119 Explore: 0.05\n",
      "i:  181\n",
      "Episode: 181  Reward: -119 Explore: 0.05\n",
      "i:  182\n",
      "Episode: 182  Reward: -119 Explore: 0.05\n",
      "i:  183\n",
      "Episode: 183  Reward: -119 Explore: 0.05\n",
      "i:  184\n",
      "Episode: 184  Reward: -119 Explore: 0.05\n",
      "i:  185\n",
      "Episode: 185  Reward: -119 Explore: 0.05\n",
      "i:  186\n",
      "Episode: 186  Reward: -119 Explore: 0.05\n",
      "i:  187\n",
      "Episode: 187  Reward: -119 Explore: 0.04\n",
      "i:  188\n",
      "Episode: 188  Reward: -119 Explore: 0.04\n",
      "i:  189\n",
      "Episode: 189  Reward: -119 Explore: 0.04\n",
      "i:  190\n",
      "Episode: 190  Reward: -119 Explore: 0.04\n",
      "i:  191\n",
      "Episode: 191  Reward: -119 Explore: 0.04\n",
      "i:  192\n",
      "Episode: 192  Reward: -119 Explore: 0.04\n",
      "i:  193\n",
      "Episode: 193  Reward: -119 Explore: 0.04\n",
      "i:  194\n",
      "Episode: 194  Reward: -119 Explore: 0.04\n",
      "i:  195\n",
      "Episode: 195  Reward: -119 Explore: 0.04\n",
      "i:  196\n",
      "Episode: 196  Reward: -119 Explore: 0.04\n",
      "i:  197\n",
      "Episode: 197  Reward: -119 Explore: 0.04\n",
      "i:  198\n",
      "Episode: 198  Reward: -119 Explore: 0.03\n",
      "i:  199\n",
      "Episode: 199  Reward: -119 Explore: 0.03\n"
     ]
    }
   ],
   "source": [
    "for i in range(MAX_EPISODES):\n",
    "    print(\"i: \", i)\n",
    "    s = env.reset()\n",
    "    ep_reward = 0\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "        # Add exploration noise\n",
    "        a = ddpg.choose_action(s)\n",
    "        # a = np.clip(np.random.normal(a, var), -2, 2)    # add randomness to action selection for exploration\n",
    "        # print(\"a: \", a)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        # print(\"r: \", r)\n",
    "        ddpg.store_transition(s, a, r, s_)\n",
    "\n",
    "        # if ddpg.pointer > MEMORY_CAPACITY:\n",
    "        #     ddpg.var *= .9995    # decay the action randomness\n",
    "        #     ddpg.learn()\n",
    "\n",
    "        s = s_\n",
    "        ep_reward += r\n",
    "\n",
    "        \n",
    "\n",
    "        if j == MAX_EP_STEPS-1:\n",
    "            r_save.append(ep_reward)\n",
    "            print('Episode:', i, ' Reward: %i' % int(ep_reward), 'Explore: %.2f' % ddpg.var, )\n",
    "            # if ep_reward > -300:RENDER = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save(\"memory-action_dim5-norm\", ddpg.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reward = r_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhILP8f8gS4IOh3WEQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa484073550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.plot([1, 2, 3, 4], [10, 20, 25, 30], color='lightblue', linewidth=3) # plot line\n",
    "ax.scatter(range(len(all_reward)), all_reward, color='green', marker='^') # plot points\n",
    "# ax.set_xlim(0.5, 4.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
