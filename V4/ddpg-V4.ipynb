{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">self.action_dim = 5\n",
    "\n",
    "        a = self.sess.run(self.a, {self.S: s[np.newaxis, :]})[0]\n",
    "        a = np.clip(np.random.normal(a, self.var), -2, 2)\n",
    "        \n",
    "        a *= mem_std[self.s_dim: self.s_dim + self.a_dim]\n",
    "        a += mem_mean[self.s_dim: self.s_dim + self.a_dim]\n",
    "        \n",
    "Adjust to:\n",
    "\n",
    "        a = self.sess.run(self.a, {self.S: s[np.newaxis, :]})[0]\n",
    "        \n",
    "        a *= mem_std[self.s_dim: self.s_dim + self.a_dim]\n",
    "        a += mem_mean[self.s_dim: self.s_dim + self.a_dim]        \n",
    "        \n",
    "        a = np.clip(np.random.normal(a, self.var), -2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "View more on the tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import manipulator\n",
    "\n",
    "# reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "\n",
    "MAX_EP_STEPS = 500\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.002    # learning rate for critic\n",
    "GAMMA = 0.9     # reward discount\n",
    "TAU = 0.01      # soft replacement\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "RENDER = False\n",
    "\n",
    "###############################  DDPG  ####################################\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound,):\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "        self.pointer = 0\n",
    "        self.sess = tf.Session()\n",
    "        self.var = 3.0\n",
    "        # self.a_replace_counter, self.c_replace_counter = 0, 0\n",
    "\n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,\n",
    "        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'r')\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.a = self.build_a_nn(self.S, scope='eval', trainable=True)\n",
    "            a_ = self.build_a_nn(self.S_, scope='target', trainable=False)\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # assign self.a = a in memory when calculating q for td_error,\n",
    "            # otherwise the self.a is from Actor when updating Actor\n",
    "            q = self.build_c_nn(self.S, self.a, scope='eval', trainable=True)\n",
    "            q_ = self.build_c_nn(self.S_, a_, scope='target', trainable=False)\n",
    "\n",
    "        # networks parameters\n",
    "        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        # target net replacement\n",
    "        self.soft_replace = [[tf.assign(at, (1-TAU)*at+TAU*ae), tf.assign(ct, (1-TAU)*ct+TAU*ce)]\n",
    "            for at, ae, ct, ce in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]\n",
    "\n",
    "        q_target = self.R + GAMMA * q_\n",
    "        # in the feed_dic for the td_error, the self.a should change to actions in memory\n",
    "        td_error = tf.losses.mean_squared_error(labels=(self.R + GAMMA * q_), predictions=q)\n",
    "        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, name=\"adam-ink\", var_list = self.ce_params)\n",
    "\n",
    "        a_loss = - tf.reduce_mean(q)    # maximize the q\n",
    "        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.ae_params)\n",
    "\n",
    "        tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "       \n",
    "\n",
    "    def choose_action(self, s):\n",
    "        \n",
    "        mem_mean = np.array([ 0.48621982, -0.39433715,  0.55701429,  0.71694756,  0.93387568,\n",
    "        0.37078592, -0.14167269,  0.40072495,  0.25386503,  0.4805119 ,\n",
    "        0.09596774, -0.11102104,  0.16225703,  0.06218311,  0.19568023,\n",
    "       -0.26812184,  0.48575708, -0.39573932,  0.55680048,  0.71941608,\n",
    "        0.93555212,  0.37175739, -0.14246416,  0.40183851,  0.25530386,\n",
    "        0.48238301], dtype=np.float32)\n",
    "        \n",
    "        mem_std = np.array([ 0.77474898,  0.72022182,  0.56043774,  0.90410036,  0.12644294,\n",
    "        0.24395365,  0.3367165 ,  0.20749338,  0.49386111,  0.27530244,\n",
    "        0.13662679,  0.10744574,  0.08928268,  0.18348816,  0.02752163,\n",
    "        0.06287373,  0.77596879,  0.72043914,  0.56105018,  0.90423542,\n",
    "        0.11954755,  0.24412741,  0.3376236 ,  0.20702067,  0.49511331,\n",
    "        0.27521592], dtype=np.float32)    \n",
    "    \n",
    "    \n",
    "        s -= mem_mean[:self.s_dim]\n",
    "        s /= mem_std[:self.s_dim]\n",
    "        \n",
    "        a = self.sess.run(self.a, {self.S: s[np.newaxis, :]})[0]\n",
    "        \n",
    "        a *= mem_std[self.s_dim: self.s_dim + self.a_dim]\n",
    "        a += mem_mean[self.s_dim: self.s_dim + self.a_dim]\n",
    "        \n",
    "        a = np.clip(np.random.normal(a, self.var), -2, 2)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def learn(self):\n",
    "        # soft target replacement\n",
    "        self.sess.run(self.soft_replace)\n",
    "\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        bt = self.memory[indices, :]\n",
    "        bs = bt[:, :self.s_dim]\n",
    "        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]\n",
    "        br = bt[:, -self.s_dim - 1: -self.s_dim]\n",
    "        bs_ = bt[:, -self.s_dim:]\n",
    "\n",
    "        self.sess.run(self.atrain, {self.S: bs})\n",
    "        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "\n",
    "        trans = np.hstack((s,a,[r],s_))\n",
    "        \n",
    "        # batch normalization\n",
    "        mem_mean = np.array([ 0.48621982, -0.39433715,  0.55701429,  0.71694756,  0.93387568,\n",
    "        0.37078592, -0.14167269,  0.40072495,  0.25386503,  0.4805119 ,\n",
    "        0.09596774, -0.11102104,  0.16225703,  0.06218311,  0.19568023,\n",
    "       -0.26812184,  0.48575708, -0.39573932,  0.55680048,  0.71941608,\n",
    "        0.93555212,  0.37175739, -0.14246416,  0.40183851,  0.25530386,\n",
    "        0.48238301], dtype=np.float32)\n",
    "        \n",
    "        mem_std = np.array([ 0.77474898,  0.72022182,  0.56043774,  0.90410036,  0.12644294,\n",
    "        0.24395365,  0.3367165 ,  0.20749338,  0.49386111,  0.27530244,\n",
    "        0.13662679,  0.10744574,  0.08928268,  0.18348816,  0.02752163,\n",
    "        0.06287373,  0.77596879,  0.72043914,  0.56105018,  0.90423542,\n",
    "        0.11954755,  0.24412741,  0.3376236 ,  0.20702067,  0.49511331,\n",
    "        0.27521592], dtype=np.float32)    \n",
    "        \n",
    "        trans -= mem_mean\n",
    "        trans /= mem_std\n",
    "        \n",
    "        \n",
    "        # print(\"trans: \", trans)\n",
    "        index = self.pointer % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = trans\n",
    "        self.pointer += 1\n",
    "\n",
    "        if self.pointer > MEMORY_CAPACITY:\n",
    "            self.var *= 0.99995\n",
    "            self.learn()\n",
    "    def build_a_nn(self, s, scope, trainable):\n",
    "        # Actor DPG\n",
    "        with tf.variable_scope(scope):\n",
    "            l1 = tf.layers.dense(s, 30, activation = tf.nn.tanh, name = 'l1', trainable = trainable)\n",
    "            a = tf.layers.dense(l1, self.a_dim, activation = tf.nn.tanh, name = 'a', trainable = trainable)     \n",
    "            return tf.multiply(a, self.a_bound, name = \"scaled_a\")  \n",
    "    # def _build_c(self, s, a, scope, trainable):\n",
    "    #     with tf.variable_scope(scope):\n",
    "    #         n_l1 = 30\n",
    "    #         w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable)\n",
    "    #         w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable)\n",
    "    #         b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n",
    "    #         net = tf.nn.tanh(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "    #         return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)\n",
    "    def build_c_nn(self, s, a, scope, trainable):\n",
    "        # Critic Q-leaning\n",
    "        with tf.variable_scope(scope):\n",
    "            n_l1 = 30\n",
    "            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable = trainable)\n",
    "            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable = trainable)\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], trainable = trainable)\n",
    "            net = tf.nn.tanh( tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1 )\n",
    "\n",
    "            q = tf.layers.dense(net, 1, trainable = trainable)\n",
    "            return q\n",
    "\n",
    "    \n",
    "###############################  training  ####################################\n",
    "\n",
    "env = manipulator.manipulator()\n",
    "# env = env.unwrapped\n",
    "# env.seed(1)\n",
    "\n",
    "s_dim = env.state_dim\n",
    "a_dim = env.action_dim\n",
    "a_bound = 0.2\n",
    "\n",
    "ddpg = DDPG(a_dim, s_dim, a_bound)\n",
    "\n",
    "r_save = []\n",
    "\n",
    "# var = 3  # control exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_save = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "Episode: 0  Reward: -175 Explore: 3.00\n",
      "i:  1\n",
      "Episode: 1  Reward: -178 Explore: 3.00\n",
      "i:  2\n",
      "Episode: 2  Reward: -181 Explore: 3.00\n",
      "i:  3\n",
      "Episode: 3  Reward: -179 Explore: 3.00\n",
      "i:  4\n",
      "Episode: 4  Reward: -179 Explore: 3.00\n",
      "i:  5\n",
      "Episode: 5  Reward: -179 Explore: 3.00\n",
      "i:  6\n",
      "Episode: 6  Reward: -177 Explore: 3.00\n",
      "i:  7\n",
      "Episode: 7  Reward: -180 Explore: 3.00\n",
      "i:  8\n",
      "Episode: 8  Reward: -184 Explore: 3.00\n",
      "i:  9\n",
      "Episode: 9  Reward: -178 Explore: 3.00\n",
      "i:  10\n",
      "Episode: 10  Reward: -177 Explore: 3.00\n",
      "i:  11\n",
      "Episode: 11  Reward: -184 Explore: 3.00\n",
      "i:  12\n",
      "Episode: 12  Reward: -178 Explore: 3.00\n",
      "i:  13\n",
      "Episode: 13  Reward: -179 Explore: 3.00\n",
      "i:  14\n",
      "Episode: 14  Reward: -179 Explore: 3.00\n",
      "i:  15\n",
      "Episode: 15  Reward: -183 Explore: 3.00\n",
      "i:  16\n",
      "Episode: 16  Reward: -182 Explore: 3.00\n",
      "i:  17\n",
      "Episode: 17  Reward: -177 Explore: 3.00\n",
      "i:  18\n",
      "Episode: 18  Reward: -181 Explore: 3.00\n",
      "i:  19\n",
      "Episode: 19  Reward: -181 Explore: 3.00\n",
      "i:  20\n",
      "Episode: 20  Reward: -180 Explore: 2.93\n",
      "i:  21\n",
      "Episode: 21  Reward: -181 Explore: 2.85\n",
      "i:  22\n",
      "Episode: 22  Reward: -182 Explore: 2.78\n",
      "i:  23\n",
      "Episode: 23  Reward: -183 Explore: 2.71\n",
      "i:  24\n",
      "Episode: 24  Reward: -179 Explore: 2.65\n",
      "i:  25\n",
      "Episode: 25  Reward: -181 Explore: 2.58\n",
      "i:  26\n",
      "Episode: 26  Reward: -183 Explore: 2.52\n",
      "i:  27\n",
      "Episode: 27  Reward: -180 Explore: 2.46\n",
      "i:  28\n",
      "Episode: 28  Reward: -183 Explore: 2.40\n",
      "i:  29\n",
      "Episode: 29  Reward: -177 Explore: 2.34\n",
      "i:  30\n",
      "Episode: 30  Reward: -176 Explore: 2.28\n",
      "i:  31\n",
      "Episode: 31  Reward: -183 Explore: 2.22\n",
      "i:  32\n",
      "Episode: 32  Reward: -181 Explore: 2.17\n",
      "i:  33\n",
      "Episode: 33  Reward: -179 Explore: 2.11\n",
      "i:  34\n",
      "Episode: 34  Reward: -178 Explore: 2.06\n",
      "i:  35\n",
      "Episode: 35  Reward: -180 Explore: 2.01\n",
      "i:  36\n",
      "Episode: 36  Reward: -178 Explore: 1.96\n",
      "i:  37\n",
      "Episode: 37  Reward: -175 Explore: 1.91\n",
      "i:  38\n",
      "Episode: 38  Reward: -181 Explore: 1.87\n",
      "i:  39\n",
      "Episode: 39  Reward: -178 Explore: 1.82\n",
      "i:  40\n",
      "Episode: 40  Reward: -179 Explore: 1.77\n",
      "i:  41\n",
      "Episode: 41  Reward: -177 Explore: 1.73\n",
      "i:  42\n",
      "Episode: 42  Reward: -177 Explore: 1.69\n",
      "i:  43\n",
      "Episode: 43  Reward: -181 Explore: 1.65\n",
      "i:  44\n",
      "Episode: 44  Reward: -178 Explore: 1.61\n",
      "i:  45\n",
      "Episode: 45  Reward: -172 Explore: 1.57\n",
      "i:  46\n",
      "Episode: 46  Reward: -175 Explore: 1.53\n",
      "i:  47\n",
      "Episode: 47  Reward: -172 Explore: 1.49\n",
      "i:  48\n",
      "Episode: 48  Reward: -172 Explore: 1.45\n",
      "i:  49\n",
      "Episode: 49  Reward: -179 Explore: 1.42\n",
      "i:  50\n",
      "Episode: 50  Reward: -180 Explore: 1.38\n",
      "i:  51\n",
      "Episode: 51  Reward: -176 Explore: 1.35\n",
      "i:  52\n",
      "Episode: 52  Reward: -176 Explore: 1.31\n",
      "i:  53\n",
      "Episode: 53  Reward: -169 Explore: 1.28\n",
      "i:  54\n",
      "Episode: 54  Reward: -171 Explore: 1.25\n",
      "i:  55\n",
      "Episode: 55  Reward: -169 Explore: 1.22\n",
      "i:  56\n",
      "Episode: 56  Reward: -171 Explore: 1.19\n",
      "i:  57\n",
      "Episode: 57  Reward: -173 Explore: 1.16\n",
      "i:  58\n",
      "Episode: 58  Reward: -177 Explore: 1.13\n",
      "i:  59\n",
      "Episode: 59  Reward: -176 Explore: 1.10\n",
      "i:  60\n",
      "Episode: 60  Reward: -171 Explore: 1.08\n",
      "i:  61\n",
      "Episode: 61  Reward: -170 Explore: 1.05\n",
      "i:  62\n",
      "Episode: 62  Reward: -166 Explore: 1.02\n",
      "i:  63\n",
      "Episode: 63  Reward: -166 Explore: 1.00\n",
      "i:  64\n",
      "Episode: 64  Reward: -169 Explore: 0.97\n",
      "i:  65\n",
      "Episode: 65  Reward: -169 Explore: 0.95\n",
      "i:  66\n",
      "Episode: 66  Reward: -173 Explore: 0.93\n",
      "i:  67\n",
      "Episode: 67  Reward: -162 Explore: 0.90\n",
      "i:  68\n",
      "Episode: 68  Reward: -165 Explore: 0.88\n",
      "i:  69\n",
      "Episode: 69  Reward: -165 Explore: 0.86\n",
      "i:  70\n",
      "Episode: 70  Reward: -165 Explore: 0.84\n",
      "i:  71\n",
      "Episode: 71  Reward: -166 Explore: 0.82\n",
      "i:  72\n",
      "Episode: 72  Reward: -164 Explore: 0.80\n",
      "i:  73\n",
      "Episode: 73  Reward: -166 Explore: 0.78\n",
      "i:  74\n",
      "Episode: 74  Reward: -161 Explore: 0.76\n",
      "i:  75\n",
      "Episode: 75  Reward: -164 Explore: 0.74\n",
      "i:  76\n",
      "Episode: 76  Reward: -164 Explore: 0.72\n",
      "i:  77\n",
      "Episode: 77  Reward: -162 Explore: 0.70\n",
      "i:  78\n",
      "Episode: 78  Reward: -155 Explore: 0.69\n",
      "i:  79\n",
      "Episode: 79  Reward: -159 Explore: 0.67\n",
      "i:  80\n",
      "Episode: 80  Reward: -163 Explore: 0.65\n",
      "i:  81\n",
      "Episode: 81  Reward: -158 Explore: 0.64\n",
      "i:  82\n",
      "Episode: 82  Reward: -156 Explore: 0.62\n",
      "i:  83\n",
      "Episode: 83  Reward: -149 Explore: 0.61\n",
      "i:  84\n",
      "Episode: 84  Reward: -161 Explore: 0.59\n",
      "i:  85\n",
      "Episode: 85  Reward: -155 Explore: 0.58\n",
      "i:  86\n",
      "Episode: 86  Reward: -153 Explore: 0.56\n",
      "i:  87\n",
      "Episode: 87  Reward: -157 Explore: 0.55\n",
      "i:  88\n",
      "Episode: 88  Reward: -157 Explore: 0.53\n",
      "i:  89\n",
      "Episode: 89  Reward: -148 Explore: 0.52\n",
      "i:  90\n",
      "Episode: 90  Reward: -152 Explore: 0.51\n",
      "i:  91\n",
      "Episode: 91  Reward: -150 Explore: 0.50\n",
      "i:  92\n",
      "Episode: 92  Reward: -156 Explore: 0.48\n",
      "i:  93\n",
      "Episode: 93  Reward: -151 Explore: 0.47\n",
      "i:  94\n",
      "Episode: 94  Reward: -154 Explore: 0.46\n",
      "i:  95\n",
      "Episode: 95  Reward: -147 Explore: 0.45\n",
      "i:  96\n",
      "Episode: 96  Reward: -144 Explore: 0.44\n",
      "i:  97\n",
      "Episode: 97  Reward: -143 Explore: 0.43\n",
      "i:  98\n",
      "Episode: 98  Reward: -145 Explore: 0.42\n",
      "i:  99\n",
      "Episode: 99  Reward: -144 Explore: 0.41\n",
      "i:  100\n",
      "Episode: 100  Reward: -145 Explore: 0.40\n",
      "i:  101\n",
      "Episode: 101  Reward: -153 Explore: 0.39\n",
      "i:  102\n",
      "Episode: 102  Reward: -148 Explore: 0.38\n",
      "i:  103\n",
      "Episode: 103  Reward: -145 Explore: 0.37\n",
      "i:  104\n",
      "Episode: 104  Reward: -146 Explore: 0.36\n",
      "i:  105\n",
      "Episode: 105  Reward: -153 Explore: 0.35\n",
      "i:  106\n",
      "Episode: 106  Reward: -143 Explore: 0.34\n",
      "i:  107\n",
      "Episode: 107  Reward: -141 Explore: 0.33\n",
      "i:  108\n",
      "Episode: 108  Reward: -144 Explore: 0.32\n",
      "i:  109\n",
      "Episode: 109  Reward: -144 Explore: 0.32\n",
      "i:  110\n",
      "Episode: 110  Reward: -141 Explore: 0.31\n",
      "i:  111\n",
      "Episode: 111  Reward: -137 Explore: 0.30\n",
      "i:  112\n",
      "Episode: 112  Reward: -144 Explore: 0.29\n",
      "i:  113\n",
      "Episode: 113  Reward: -135 Explore: 0.29\n",
      "i:  114\n",
      "Episode: 114  Reward: -148 Explore: 0.28\n",
      "i:  115\n",
      "Episode: 115  Reward: -143 Explore: 0.27\n",
      "i:  116\n",
      "Episode: 116  Reward: -134 Explore: 0.27\n",
      "i:  117\n",
      "Episode: 117  Reward: -132 Explore: 0.26\n",
      "i:  118\n",
      "Episode: 118  Reward: -139 Explore: 0.25\n",
      "i:  119\n",
      "Episode: 119  Reward: -136 Explore: 0.25\n",
      "i:  120\n",
      "Episode: 120  Reward: -134 Explore: 0.24\n",
      "i:  121\n",
      "Episode: 121  Reward: -134 Explore: 0.23\n",
      "i:  122\n",
      "Episode: 122  Reward: -133 Explore: 0.23\n",
      "i:  123\n",
      "Episode: 123  Reward: -131 Explore: 0.22\n",
      "i:  124\n",
      "Episode: 124  Reward: -135 Explore: 0.22\n",
      "i:  125\n",
      "Episode: 125  Reward: -133 Explore: 0.21\n",
      "i:  126\n",
      "Episode: 126  Reward: -131 Explore: 0.21\n",
      "i:  127\n",
      "Episode: 127  Reward: -131 Explore: 0.20\n",
      "i:  128\n",
      "Episode: 128  Reward: -134 Explore: 0.20\n",
      "i:  129\n",
      "Episode: 129  Reward: -132 Explore: 0.19\n",
      "i:  130\n",
      "Episode: 130  Reward: -129 Explore: 0.19\n",
      "i:  131\n",
      "Episode: 131  Reward: -125 Explore: 0.18\n",
      "i:  132\n",
      "Episode: 132  Reward: -127 Explore: 0.18\n",
      "i:  133\n",
      "Episode: 133  Reward: -125 Explore: 0.17\n",
      "i:  134\n",
      "Episode: 134  Reward: -131 Explore: 0.17\n",
      "i:  135\n",
      "Episode: 135  Reward: -126 Explore: 0.17\n",
      "i:  136\n",
      "Episode: 136  Reward: -133 Explore: 0.16\n",
      "i:  137\n",
      "Episode: 137  Reward: -125 Explore: 0.16\n",
      "i:  138\n",
      "Episode: 138  Reward: -124 Explore: 0.15\n",
      "i:  139\n",
      "Episode: 139  Reward: -124 Explore: 0.15\n",
      "i:  140\n",
      "Episode: 140  Reward: -124 Explore: 0.15\n",
      "i:  141\n",
      "Episode: 141  Reward: -125 Explore: 0.14\n",
      "i:  142\n",
      "Episode: 142  Reward: -122 Explore: 0.14\n",
      "i:  143\n",
      "Episode: 143  Reward: -122 Explore: 0.14\n",
      "i:  144\n",
      "Episode: 144  Reward: -123 Explore: 0.13\n",
      "i:  145\n",
      "Episode: 145  Reward: -122 Explore: 0.13\n",
      "i:  146\n",
      "Episode: 146  Reward: -121 Explore: 0.13\n",
      "i:  147\n",
      "Episode: 147  Reward: -121 Explore: 0.12\n",
      "i:  148\n",
      "Episode: 148  Reward: -121 Explore: 0.12\n",
      "i:  149\n",
      "Episode: 149  Reward: -120 Explore: 0.12\n",
      "i:  150\n",
      "Episode: 150  Reward: -120 Explore: 0.11\n",
      "i:  151\n",
      "Episode: 151  Reward: -119 Explore: 0.11\n",
      "i:  152\n",
      "Episode: 152  Reward: -120 Explore: 0.11\n",
      "i:  153\n",
      "Episode: 153  Reward: -120 Explore: 0.11\n",
      "i:  154\n",
      "Episode: 154  Reward: -119 Explore: 0.10\n",
      "i:  155\n",
      "Episode: 155  Reward: -121 Explore: 0.10\n",
      "i:  156\n",
      "Episode: 156  Reward: -119 Explore: 0.10\n",
      "i:  157\n",
      "Episode: 157  Reward: -123 Explore: 0.10\n",
      "i:  158\n",
      "Episode: 158  Reward: -122 Explore: 0.09\n",
      "i:  159\n",
      "Episode: 159  Reward: -119 Explore: 0.09\n",
      "i:  160\n",
      "Episode: 160  Reward: -120 Explore: 0.09\n",
      "i:  161\n",
      "Episode: 161  Reward: -119 Explore: 0.09\n",
      "i:  162\n",
      "Episode: 162  Reward: -119 Explore: 0.08\n",
      "i:  163\n",
      "Episode: 163  Reward: -119 Explore: 0.08\n",
      "i:  164\n",
      "Episode: 164  Reward: -119 Explore: 0.08\n",
      "i:  165\n",
      "Episode: 165  Reward: -120 Explore: 0.08\n",
      "i:  166\n",
      "Episode: 166  Reward: -120 Explore: 0.08\n",
      "i:  167\n",
      "Episode: 167  Reward: -120 Explore: 0.07\n",
      "i:  168\n",
      "Episode: 168  Reward: -120 Explore: 0.07\n",
      "i:  169\n",
      "Episode: 169  Reward: -120 Explore: 0.07\n",
      "i:  170\n",
      "Episode: 170  Reward: -121 Explore: 0.07\n",
      "i:  171\n",
      "Episode: 171  Reward: -119 Explore: 0.07\n",
      "i:  172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 172  Reward: -120 Explore: 0.07\n",
      "i:  173\n",
      "Episode: 173  Reward: -120 Explore: 0.06\n",
      "i:  174\n",
      "Episode: 174  Reward: -119 Explore: 0.06\n",
      "i:  175\n",
      "Episode: 175  Reward: -119 Explore: 0.06\n",
      "i:  176\n",
      "Episode: 176  Reward: -119 Explore: 0.06\n",
      "i:  177\n",
      "Episode: 177  Reward: -119 Explore: 0.06\n",
      "i:  178\n",
      "Episode: 178  Reward: -120 Explore: 0.06\n",
      "i:  179\n",
      "Episode: 179  Reward: -119 Explore: 0.05\n",
      "i:  180\n",
      "Episode: 180  Reward: -119 Explore: 0.05\n",
      "i:  181\n",
      "Episode: 181  Reward: -119 Explore: 0.05\n",
      "i:  182\n",
      "Episode: 182  Reward: -120 Explore: 0.05\n",
      "i:  183\n",
      "Episode: 183  Reward: -119 Explore: 0.05\n",
      "i:  184\n",
      "Episode: 184  Reward: -119 Explore: 0.05\n",
      "i:  185\n",
      "Episode: 185  Reward: -120 Explore: 0.05\n",
      "i:  186\n",
      "Episode: 186  Reward: -119 Explore: 0.05\n",
      "i:  187\n",
      "Episode: 187  Reward: -119 Explore: 0.04\n",
      "i:  188\n",
      "Episode: 188  Reward: -120 Explore: 0.04\n",
      "i:  189\n",
      "Episode: 189  Reward: -120 Explore: 0.04\n",
      "i:  190\n",
      "Episode: 190  Reward: -120 Explore: 0.04\n",
      "i:  191\n",
      "Episode: 191  Reward: -120 Explore: 0.04\n",
      "i:  192\n",
      "Episode: 192  Reward: -119 Explore: 0.04\n",
      "i:  193\n",
      "Episode: 193  Reward: -119 Explore: 0.04\n",
      "i:  194\n",
      "Episode: 194  Reward: -120 Explore: 0.04\n",
      "i:  195\n",
      "Episode: 195  Reward: -119 Explore: 0.04\n",
      "i:  196\n",
      "Episode: 196  Reward: -120 Explore: 0.04\n",
      "i:  197\n",
      "Episode: 197  Reward: -119 Explore: 0.04\n",
      "i:  198\n",
      "Episode: 198  Reward: -119 Explore: 0.03\n",
      "i:  199\n",
      "Episode: 199  Reward: -119 Explore: 0.03\n"
     ]
    }
   ],
   "source": [
    "for i in range(MAX_EPISODES):\n",
    "    print(\"i: \", i)\n",
    "    s = env.reset()\n",
    "    ep_reward = 0\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "        # Add exploration noise\n",
    "        a = ddpg.choose_action(s)\n",
    "        # a = np.clip(np.random.normal(a, var), -2, 2)    # add randomness to action selection for exploration\n",
    "        # print(\"a: \", a)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        # print(\"r: \", r)\n",
    "        ddpg.store_transition(s, a, r, s_)\n",
    "\n",
    "        # if ddpg.pointer > MEMORY_CAPACITY:\n",
    "        #     ddpg.var *= .9995    # decay the action randomness\n",
    "        #     ddpg.learn()\n",
    "\n",
    "        s = s_\n",
    "        ep_reward += r\n",
    "\n",
    "        \n",
    "\n",
    "        if j == MAX_EP_STEPS-1:\n",
    "            r_save.append(ep_reward)\n",
    "            print('Episode:', i, ' Reward: %i' % int(ep_reward), 'Explore: %.2f' % ddpg.var, )\n",
    "            # if ep_reward > -300:RENDER = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save(\"memory-action_dim5-norm\", ddpg.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reward = r_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noGAgIiLA/wfO3MmrbsNJmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f59c7b20470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.plot([1, 2, 3, 4], [10, 20, 25, 30], color='lightblue', linewidth=3) # plot line\n",
    "ax.scatter(range(len(all_reward)), all_reward, color='green', marker='^') # plot points\n",
    "# ax.set_xlim(0.5, 4.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
