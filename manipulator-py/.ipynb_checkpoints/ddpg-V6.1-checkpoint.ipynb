{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "View more on the tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import manipulator\n",
    "\n",
    "# # # reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "\n",
    "MAX_EP_STEPS = 500\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.002    # learning rate for critic\n",
    "GAMMA = 0.9     # reward discount\n",
    "TAU = 0.01      # soft replacement\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "RENDER = False\n",
    "\n",
    "###############################  DDPG  ####################################\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound,):\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "        self.pointer = 0\n",
    "        self.sess = tf.Session()\n",
    "                \n",
    "        if TRAIN:\n",
    "            self.var = 3.0\n",
    "        else:\n",
    "            self.var = 0.0        \n",
    "            \n",
    "#         self.mean = np.array([0.]*26, dtype=np.float32)\n",
    "        \n",
    "#         self.std = np.array([1.]*26, dtype=np.float32)    \n",
    "        \n",
    "#         self.sum_ai2 = np.array([0.]*26, dtype=np.float32)\n",
    "\n",
    "#         self.mean = np.array( [0.34834658743584485, -0.6318716911473438, -0.5661514854517639, 0.07621472819056746, 0.028013053053513273, 0.20609833036457584, \n",
    "#         -0.3062108267781698, -0.3024539663611043, 0.06093039756831196, 0.033963509642047904, 0.04108029979757066, -0.06115245273286279, \n",
    "#         -0.057278341082980654, -0.01868992182555732, 0.016859964070466027, -0.20114772570077774, 0.4842639725240364, -0.8196236115959136, \n",
    "#         -0.7503603561777605, -0.061486437144056924, 0.07334224875503308, 0.22888182549791716, -0.4030543421278879, -0.36736219691606675, \n",
    "#         -0.03777272967640111, 0.059554004128217874] )\n",
    "#         self.std = np.array( [0.802520082706222, 0.4618768626366726, 0.5814252385481398, 0.9310726271789103, 0.9588982301233124, \n",
    "#         0.5896576086242203, 0.5318602082562012, 0.5521094460520812, 0.668038191808423, 0.6829795959253061, 0.16029570691706538, 0.16024717414553674, \n",
    "#         0.1631756828059499, 0.16218674224861002, 0.1620384051283602, 0.07192591718283593, 0.7040880395342896, 0.4223013893411566, 0.5288619487888918, \n",
    "#         0.8427154097305507, 0.8778329454814024, 0.3436760212947522, 0.3000819712667782, 0.3300498255881239, 0.4264838047763872, 0.42893611608961796] )\n",
    "\n",
    "        \n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,\n",
    "        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'r')\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.a = self.build_a_nn(self.S, scope='eval', trainable=True)\n",
    "            a_ = self.build_a_nn(self.S_, scope='target', trainable=True)\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # assign self.a = a in memory when calculating q for td_error,\n",
    "            # otherwise the self.a is from Actor when updating Actor\n",
    "            q = self.build_c_nn(self.S, self.a, scope='eval', trainable=True)\n",
    "            q_ = self.build_c_nn(self.S_, a_, scope='target', trainable=True)\n",
    "\n",
    "        # networks parameters\n",
    "        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        # target net replacement\n",
    "        self.soft_replace = [[tf.assign(at, (1-TAU)*at+TAU*ae), tf.assign(ct, (1-TAU)*ct+TAU*ce)]\n",
    "            for at, ae, ct, ce in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]\n",
    "\n",
    "        q_target = self.R + GAMMA * q_\n",
    "        # in the feed_dic for the td_error, the self.a should change to actions in memory\n",
    "        td_error = tf.losses.mean_squared_error(labels=(self.R + GAMMA * q_), predictions=q)\n",
    "        \n",
    "        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, name=\"adam-ink\", var_list = self.ce_params)\n",
    "\n",
    "        a_loss = - tf.reduce_mean(q)    # maximize the q\n",
    "        \n",
    "        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.ae_params)\n",
    "\n",
    "        tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.model_path = \"./model_path/\"\n",
    "        \n",
    "        if TRAIN:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            load_path = self.saver.restore(self.sess, self.model_path)\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        \n",
    "#         self.mean = np.array([-0.30596643686294556, 0.6531972885131836, 0.07150639593601227, \n",
    "#                              -0.4814097285270691, -0.9779564738273621, -0.1355571746826172, \n",
    "#         0.361149400472641, 0.1673995554447174, -0.33002352714538574, -0.4877714216709137, -0.3059367537498474, 0.6378231644630432, \n",
    "#         0.1766030341386795, -0.7159786820411682, -0.9878270626068115, 0.19998057186603546, -0.19998057186603546, 0.09049104899168015, \n",
    "#         -0.052024029195308685, 0.12978997826576233, 0.02930569089949131, -0.1457349807024002, -0.1941746175289154, -0.19394275546073914, \n",
    "#         -0.3071609139442444, 0.654149055480957, 0.07036503404378891, -0.4808672368526459, -0.9798941612243652, -0.13617150485515594, \n",
    "#         0.3624577522277832, 0.1675402969121933, -0.33098533749580383, -0.4897313117980957, -0.3071301579475403, 0.6387059688568115, \n",
    "#         0.17597635090351105, -0.7166529297828674, -0.989827036857605, 0.19998057186603546, \n",
    "#                              -0.19998057186603546, 0.09049104899168015] , dtype=np.float32)\n",
    "        \n",
    "#         self.std = np.array([0.8033351898193359, 0.5680350065231323, 0.8028296232223511, \n",
    "#                             0.789441704750061, 0.08707397431135178, 0.40394535660743713, \n",
    "#         0.28473415970802307, 0.34262776374816895, 0.3564625084400177, 0.2858351767063141, 0.8037617206573486, 0.5734468102455139, \n",
    "#         0.856696605682373, 0.6527989506721497, 0.08459825068712234, 1.9432218323345296e-05, 1.9432218323345296e-05, 0.028779448941349983, \n",
    "#         0.16029109060764313, 0.11157218366861343, 0.17511923611164093, 0.13185659050941467, 0.05517452210187912, 0.05518483370542526, \n",
    "#         0.8039249181747437, 0.568067729473114, 0.8037550449371338, 0.7909086346626282, 0.07530832290649414, 0.4052116870880127, \n",
    "#         0.2851526737213135, 0.3434385359287262, 0.3570743203163147, 0.2858463227748871, 0.8042926788330078, 0.5735509395599365, \n",
    "#         0.8577484488487244, 0.6535053253173828, 0.07212276011705399, 1.9432218323345296e-05, \n",
    "#                             1.9432218323345296e-05, 0.028779448941349983], dtype=np.float32)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         if TRAIN & ( self.pointer < MEMORY_CAPACITY ):\n",
    "#             return np.clip(np.random.normal([0.]*self.a_dim, self.var), -0.2, 0.2)\n",
    "#         print(\"s 9357: \", s)\n",
    "    \n",
    "#         s -= self.mean[:self.s_dim]\n",
    "#         s /= self.std[:self.s_dim]\n",
    "\n",
    "#         s *= 10.0\n",
    "#         print(\"s 8475: \", s)\n",
    "        a = self.sess.run(self.a, {self.S: s[np.newaxis, :]})[0]\n",
    "#         a = np.clip(np.random.normal(a, self.var), -0.2, 0.2)\n",
    "\n",
    "#         a *= self.std[self.s_dim: self.s_dim + self.a_dim]\n",
    "#         a += self.mean[self.s_dim: self.s_dim + self.a_dim]\n",
    "        \n",
    "#         a /= 10.0\n",
    "#         print(\"a 2634: \", a)\n",
    "        a = np.clip(np.random.normal(a, self.var), -2, 2)\n",
    "#         print(\"a 2844: \", a)\n",
    "        return a\n",
    "\n",
    "    def learn(self):\n",
    "        # soft target replacement\n",
    "        self.sess.run(self.soft_replace)\n",
    "\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        bt = self.memory[indices, :]\n",
    "        bs = bt[:, :self.s_dim]\n",
    "        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]\n",
    "        br = bt[:, -self.s_dim - 1: -self.s_dim]\n",
    "        bs_ = bt[:, -self.s_dim:]\n",
    "\n",
    "        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})\n",
    "        self.sess.run(self.atrain, {self.S: bs, self.S_: bs_})\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "\n",
    "        trans = np.hstack((s,a,[r],s_))\n",
    "        \n",
    "#         self.mean = (self.mean*self.pointer + trans)/(self.pointer+1)\n",
    "#         self.sum_ai2 += trans**2\n",
    "#         self.std = np.sqrt( ( self.sum_ai2 -(self.pointer+1)*self.mean**2 )/(self.pointer+1) )\n",
    "\n",
    "#         self.mean = np.array([-0.30596643686294556, 0.6531972885131836, 0.07150639593601227, \n",
    "#                              -0.4814097285270691, -0.9779564738273621, -0.1355571746826172, \n",
    "#         0.361149400472641, 0.1673995554447174, -0.33002352714538574, -0.4877714216709137, -0.3059367537498474, 0.6378231644630432, \n",
    "#         0.1766030341386795, -0.7159786820411682, -0.9878270626068115, 0.19998057186603546, -0.19998057186603546, 0.09049104899168015, \n",
    "#         -0.052024029195308685, 0.12978997826576233, 0.02930569089949131, -0.1457349807024002, -0.1941746175289154, -0.19394275546073914, \n",
    "#         -0.3071609139442444, 0.654149055480957, 0.07036503404378891, -0.4808672368526459, -0.9798941612243652, -0.13617150485515594, \n",
    "#         0.3624577522277832, 0.1675402969121933, -0.33098533749580383, -0.4897313117980957, -0.3071301579475403, 0.6387059688568115, \n",
    "#         0.17597635090351105, -0.7166529297828674, -0.989827036857605, 0.19998057186603546, \n",
    "#                              -0.19998057186603546, 0.09049104899168015] , dtype=np.float32)\n",
    "        \n",
    "#         self.std = np.array([0.8033351898193359, 0.5680350065231323, 0.8028296232223511, \n",
    "#                             0.789441704750061, 0.08707397431135178, 0.40394535660743713, \n",
    "#         0.28473415970802307, 0.34262776374816895, 0.3564625084400177, 0.2858351767063141, 0.8037617206573486, 0.5734468102455139, \n",
    "#         0.856696605682373, 0.6527989506721497, 0.08459825068712234, 1.9432218323345296e-05, 1.9432218323345296e-05, 0.028779448941349983, \n",
    "#         0.16029109060764313, 0.11157218366861343, 0.17511923611164093, 0.13185659050941467, 0.05517452210187912, 0.05518483370542526, \n",
    "#         0.8039249181747437, 0.568067729473114, 0.8037550449371338, 0.7909086346626282, 0.07530832290649414, 0.4052116870880127, \n",
    "#         0.2851526737213135, 0.3434385359287262, 0.3570743203163147, 0.2858463227748871, 0.8042926788330078, 0.5735509395599365, \n",
    "#         0.8577484488487244, 0.6535053253173828, 0.07212276011705399, 1.9432218323345296e-05, \n",
    "#                             1.9432218323345296e-05, 0.028779448941349983], dtype=np.float32)    \n",
    "        \n",
    "#         trans -= self.mean\n",
    "#         trans /= self.std\n",
    "        \n",
    "#         trans *= 10.0\n",
    "        \n",
    "        # print(\"trans: \", trans)\n",
    "        index = self.pointer % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = trans\n",
    "        self.pointer += 1\n",
    "        \n",
    "        if TRAIN & ( self.pointer > MEMORY_CAPACITY ):\n",
    "#             if self.var > 1.0:\n",
    "            self.var *= 0.99995\n",
    "            self.learn()\n",
    "    def build_a_nn(self, s, scope, trainable):\n",
    "        # Actor DPG\n",
    "        with tf.variable_scope(scope):\n",
    "            s_norm = s# tf.layers.batch_normalization(s, training=self.is_training)\n",
    "            l1 = tf.layers.dense(s_norm, 30, activation = tf.nn.tanh, name = 'l1', trainable = trainable)\n",
    "            a = tf.layers.dense(l1, self.a_dim, activation = tf.nn.tanh, name = 'a', trainable = trainable)     \n",
    "            return tf.multiply(a, self.a_bound, name = \"scaled_a\")  \n",
    "    def build_c_nn(self, s, a, scope, trainable):\n",
    "        # Critic Q-leaning\n",
    "        with tf.variable_scope(scope):\n",
    "            n_l1 = 30\n",
    "            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable = trainable)\n",
    "            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable = trainable)\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], trainable = trainable)\n",
    "            \n",
    "            s_norm = s# tf.layers.batch_normalization(s, training=self.is_training)\n",
    "            a_norm = a# tf.layers.batch_normalization(a, training=self.is_training)\n",
    "            \n",
    "            linear_output = tf.matmul(s_norm, w1_s) + tf.matmul(a_norm, w1_a) + b1    \n",
    "    \n",
    "            net = tf.nn.tanh( linear_output )\n",
    "\n",
    "            q = tf.layers.dense(net, 1, trainable = trainable)\n",
    "            return q\n",
    "\n",
    "    \n",
    "###############################  training  ####################################\n",
    "\n",
    "env = manipulator.manipulator()\n",
    "\n",
    "s_dim = env.state_dim\n",
    "a_dim = env.action_dim\n",
    "a_bound = 0.2\n",
    "\n",
    "ddpg = DDPG(a_dim, s_dim, a_bound)\n",
    "\n",
    "r_save = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "Episode: 0  Reward: -110 Explore: 3.00\n",
      "i:  1\n",
      "Episode: 1  Reward: -110 Explore: 3.00\n",
      "i:  2\n",
      "Episode: 2  Reward: -102 Explore: 3.00\n",
      "i:  3\n",
      "Episode: 3  Reward: -113 Explore: 3.00\n",
      "i:  4\n",
      "Episode: 4  Reward: -106 Explore: 3.00\n",
      "i:  5\n",
      "Episode: 5  Reward: -109 Explore: 3.00\n",
      "i:  6\n",
      "Episode: 6  Reward: -104 Explore: 3.00\n",
      "i:  7\n",
      "Episode: 7  Reward: -108 Explore: 3.00\n",
      "i:  8\n",
      "Episode: 8  Reward: -99 Explore: 3.00\n",
      "i:  9\n",
      "Episode: 9  Reward: -114 Explore: 3.00\n",
      "i:  10\n",
      "Episode: 10  Reward: -102 Explore: 3.00\n",
      "i:  11\n",
      "Episode: 11  Reward: -97 Explore: 3.00\n",
      "i:  12\n",
      "Episode: 12  Reward: -111 Explore: 3.00\n",
      "i:  13\n",
      "Episode: 13  Reward: -111 Explore: 3.00\n",
      "i:  14\n",
      "Episode: 14  Reward: -109 Explore: 3.00\n",
      "i:  15\n",
      "Episode: 15  Reward: -109 Explore: 3.00\n",
      "i:  16\n",
      "Episode: 16  Reward: -103 Explore: 3.00\n",
      "i:  17\n",
      "Episode: 17  Reward: -118 Explore: 3.00\n",
      "i:  18\n",
      "Episode: 18  Reward: -102 Explore: 3.00\n",
      "i:  19\n",
      "Episode: 19  Reward: -102 Explore: 3.00\n",
      "i:  20\n",
      "Episode: 20  Reward: -111 Explore: 2.93\n",
      "i:  21\n",
      "Episode: 21  Reward: -116 Explore: 2.85\n",
      "i:  22\n",
      "Episode: 22  Reward: -111 Explore: 2.78\n",
      "i:  23\n",
      "Episode: 23  Reward: -108 Explore: 2.71\n",
      "i:  24\n",
      "Episode: 24  Reward: -102 Explore: 2.65\n",
      "i:  25\n",
      "Episode: 25  Reward: -106 Explore: 2.58\n",
      "i:  26\n",
      "Episode: 26  Reward: -103 Explore: 2.52\n",
      "i:  27\n",
      "Episode: 27  Reward: -106 Explore: 2.46\n",
      "i:  28\n",
      "Episode: 28  Reward: -94 Explore: 2.40\n",
      "i:  29\n",
      "Episode: 29  Reward: -105 Explore: 2.34\n",
      "i:  30\n",
      "Episode: 30  Reward: -103 Explore: 2.28\n",
      "i:  31\n",
      "Episode: 31  Reward: -95 Explore: 2.22\n",
      "i:  32\n",
      "Episode: 32  Reward: -99 Explore: 2.17\n",
      "i:  33\n",
      "Episode: 33  Reward: -99 Explore: 2.11\n",
      "i:  34\n",
      "Episode: 34  Reward: -100 Explore: 2.06\n",
      "i:  35\n",
      "Episode: 35  Reward: -96 Explore: 2.01\n",
      "i:  36\n",
      "Episode: 36  Reward: -101 Explore: 1.96\n",
      "i:  37\n",
      "Episode: 37  Reward: -95 Explore: 1.91\n",
      "i:  38\n",
      "Episode: 38  Reward: -92 Explore: 1.87\n",
      "i:  39\n",
      "Episode: 39  Reward: -110 Explore: 1.82\n",
      "i:  40\n",
      "Episode: 40  Reward: -104 Explore: 1.77\n",
      "i:  41\n",
      "Episode: 41  Reward: -96 Explore: 1.73\n",
      "i:  42\n",
      "Episode: 42  Reward: -96 Explore: 1.69\n",
      "i:  43\n",
      "Episode: 43  Reward: -90 Explore: 1.65\n",
      "i:  44\n",
      "Episode: 44  Reward: -94 Explore: 1.61\n",
      "i:  45\n",
      "Episode: 45  Reward: -104 Explore: 1.57\n",
      "i:  46\n",
      "Episode: 46  Reward: -94 Explore: 1.53\n",
      "i:  47\n",
      "Episode: 47  Reward: -101 Explore: 1.49\n",
      "i:  48\n",
      "Episode: 48  Reward: -96 Explore: 1.45\n",
      "i:  49\n",
      "Episode: 49  Reward: -88 Explore: 1.42\n",
      "i:  50\n",
      "Episode: 50  Reward: -94 Explore: 1.38\n",
      "i:  51\n",
      "Episode: 51  Reward: -93 Explore: 1.35\n",
      "i:  52\n",
      "Episode: 52  Reward: -91 Explore: 1.31\n",
      "i:  53\n",
      "Episode: 53  Reward: -93 Explore: 1.28\n",
      "i:  54\n",
      "Episode: 54  Reward: -88 Explore: 1.25\n",
      "i:  55\n",
      "Episode: 55  Reward: -97 Explore: 1.22\n",
      "i:  56\n",
      "Episode: 56  Reward: -92 Explore: 1.19\n",
      "i:  57\n",
      "Episode: 57  Reward: -86 Explore: 1.16\n",
      "i:  58\n",
      "Episode: 58  Reward: -85 Explore: 1.13\n",
      "i:  59\n",
      "Episode: 59  Reward: -84 Explore: 1.10\n",
      "i:  60\n",
      "Episode: 60  Reward: -86 Explore: 1.08\n",
      "i:  61\n",
      "Episode: 61  Reward: -89 Explore: 1.05\n",
      "i:  62\n",
      "Episode: 62  Reward: -87 Explore: 1.02\n",
      "i:  63\n",
      "Episode: 63  Reward: -95 Explore: 1.00\n",
      "i:  64\n",
      "Episode: 64  Reward: -88 Explore: 0.97\n",
      "i:  65\n",
      "Episode: 65  Reward: -80 Explore: 0.95\n",
      "i:  66\n",
      "Episode: 66  Reward: -76 Explore: 0.93\n",
      "i:  67\n",
      "Episode: 67  Reward: -89 Explore: 0.90\n",
      "i:  68\n",
      "Episode: 68  Reward: -86 Explore: 0.88\n",
      "i:  69\n",
      "Episode: 69  Reward: -87 Explore: 0.86\n",
      "i:  70\n",
      "Episode: 70  Reward: -76 Explore: 0.84\n",
      "i:  71\n",
      "Episode: 71  Reward: -80 Explore: 0.82\n",
      "i:  72\n",
      "Episode: 72  Reward: -80 Explore: 0.80\n",
      "i:  73\n",
      "Episode: 73  Reward: -73 Explore: 0.78\n",
      "i:  74\n",
      "Episode: 74  Reward: -80 Explore: 0.76\n",
      "i:  75\n",
      "Episode: 75  Reward: -73 Explore: 0.74\n",
      "i:  76\n",
      "Episode: 76  Reward: -67 Explore: 0.72\n",
      "i:  77\n",
      "Episode: 77  Reward: -69 Explore: 0.70\n",
      "i:  78\n",
      "Episode: 78  Reward: -69 Explore: 0.69\n",
      "i:  79\n",
      "Episode: 79  Reward: -67 Explore: 0.67\n",
      "i:  80\n",
      "Episode: 80  Reward: -73 Explore: 0.65\n",
      "i:  81\n",
      "Episode: 81  Reward: -75 Explore: 0.64\n",
      "i:  82\n",
      "Episode: 82  Reward: -67 Explore: 0.62\n",
      "i:  83\n",
      "Episode: 83  Reward: -67 Explore: 0.61\n",
      "i:  84\n",
      "Episode: 84  Reward: -74 Explore: 0.59\n",
      "i:  85\n",
      "Episode: 85  Reward: -64 Explore: 0.58\n",
      "i:  86\n",
      "Episode: 86  Reward: -70 Explore: 0.56\n",
      "i:  87\n",
      "Episode: 87  Reward: -64 Explore: 0.55\n",
      "i:  88\n",
      "Episode: 88  Reward: -59 Explore: 0.53\n",
      "i:  89\n",
      "Episode: 89  Reward: -62 Explore: 0.52\n",
      "i:  90\n",
      "Episode: 90  Reward: -59 Explore: 0.51\n",
      "i:  91\n",
      "Episode: 91  Reward: -58 Explore: 0.50\n",
      "i:  92\n",
      "Episode: 92  Reward: -53 Explore: 0.48\n",
      "i:  93\n",
      "Episode: 93  Reward: -65 Explore: 0.47\n",
      "i:  94\n",
      "Episode: 94  Reward: -58 Explore: 0.46\n",
      "i:  95\n",
      "Episode: 95  Reward: -57 Explore: 0.45\n",
      "i:  96\n",
      "Episode: 96  Reward: -56 Explore: 0.44\n",
      "i:  97\n",
      "Episode: 97  Reward: -54 Explore: 0.43\n",
      "i:  98\n",
      "Episode: 98  Reward: -52 Explore: 0.42\n",
      "i:  99\n",
      "Episode: 99  Reward: -55 Explore: 0.41\n",
      "i:  100\n",
      "Episode: 100  Reward: -52 Explore: 0.40\n",
      "i:  101\n",
      "Episode: 101  Reward: -54 Explore: 0.39\n",
      "i:  102\n",
      "Episode: 102  Reward: -54 Explore: 0.38\n",
      "i:  103\n",
      "Episode: 103  Reward: -58 Explore: 0.37\n",
      "i:  104\n",
      "Episode: 104  Reward: -58 Explore: 0.36\n",
      "i:  105\n",
      "Episode: 105  Reward: -59 Explore: 0.35\n",
      "i:  106\n",
      "Episode: 106  Reward: -67 Explore: 0.34\n",
      "i:  107\n",
      "Episode: 107  Reward: -73 Explore: 0.33\n",
      "i:  108\n",
      "Episode: 108  Reward: -68 Explore: 0.32\n",
      "i:  109\n",
      "Episode: 109  Reward: -64 Explore: 0.32\n",
      "i:  110\n",
      "Episode: 110  Reward: -65 Explore: 0.31\n",
      "i:  111\n",
      "Episode: 111  Reward: -62 Explore: 0.30\n",
      "i:  112\n",
      "Episode: 112  Reward: -62 Explore: 0.29\n",
      "i:  113\n",
      "Episode: 113  Reward: -64 Explore: 0.29\n",
      "i:  114\n",
      "Episode: 114  Reward: -58 Explore: 0.28\n",
      "i:  115\n",
      "Episode: 115  Reward: -60 Explore: 0.27\n",
      "i:  116\n",
      "Episode: 116  Reward: -62 Explore: 0.27\n",
      "i:  117\n",
      "Episode: 117  Reward: -57 Explore: 0.26\n",
      "i:  118\n",
      "Episode: 118  Reward: -60 Explore: 0.25\n",
      "i:  119\n",
      "Episode: 119  Reward: -58 Explore: 0.25\n",
      "i:  120\n",
      "Episode: 120  Reward: -57 Explore: 0.24\n",
      "i:  121\n",
      "Episode: 121  Reward: -56 Explore: 0.23\n",
      "i:  122\n",
      "Episode: 122  Reward: -55 Explore: 0.23\n",
      "i:  123\n",
      "Episode: 123  Reward: -53 Explore: 0.22\n",
      "i:  124\n",
      "Episode: 124  Reward: -56 Explore: 0.22\n",
      "i:  125\n",
      "Episode: 125  Reward: -53 Explore: 0.21\n",
      "i:  126\n",
      "Episode: 126  Reward: -53 Explore: 0.21\n",
      "i:  127\n",
      "Episode: 127  Reward: -56 Explore: 0.20\n",
      "i:  128\n",
      "Episode: 128  Reward: -58 Explore: 0.20\n",
      "i:  129\n",
      "Episode: 129  Reward: -59 Explore: 0.19\n",
      "i:  130\n",
      "Episode: 130  Reward: -52 Explore: 0.19\n",
      "i:  131\n",
      "Episode: 131  Reward: -56 Explore: 0.18\n",
      "i:  132\n",
      "Episode: 132  Reward: -55 Explore: 0.18\n",
      "i:  133\n",
      "Episode: 133  Reward: -52 Explore: 0.17\n",
      "i:  134\n",
      "Episode: 134  Reward: -56 Explore: 0.17\n",
      "i:  135\n",
      "Episode: 135  Reward: -57 Explore: 0.17\n",
      "i:  136\n",
      "Episode: 136  Reward: -56 Explore: 0.16\n",
      "i:  137\n",
      "Episode: 137  Reward: -54 Explore: 0.16\n",
      "i:  138\n",
      "Episode: 138  Reward: -54 Explore: 0.15\n",
      "i:  139\n",
      "Episode: 139  Reward: -57 Explore: 0.15\n",
      "i:  140\n",
      "Episode: 140  Reward: -59 Explore: 0.15\n",
      "i:  141\n",
      "Episode: 141  Reward: -55 Explore: 0.14\n",
      "i:  142\n",
      "Episode: 142  Reward: -51 Explore: 0.14\n",
      "i:  143\n",
      "Episode: 143  Reward: -55 Explore: 0.14\n",
      "i:  144\n",
      "Episode: 144  Reward: -51 Explore: 0.13\n",
      "i:  145\n",
      "Episode: 145  Reward: -53 Explore: 0.13\n",
      "i:  146\n",
      "Episode: 146  Reward: -53 Explore: 0.13\n",
      "i:  147\n",
      "Episode: 147  Reward: -51 Explore: 0.12\n",
      "i:  148\n",
      "Episode: 148  Reward: -57 Explore: 0.12\n",
      "i:  149\n",
      "Episode: 149  Reward: -58 Explore: 0.12\n",
      "i:  150\n",
      "Episode: 150  Reward: -59 Explore: 0.11\n",
      "i:  151\n",
      "Episode: 151  Reward: -70 Explore: 0.11\n",
      "i:  152\n",
      "Episode: 152  Reward: -51 Explore: 0.11\n",
      "i:  153\n",
      "Episode: 153  Reward: -64 Explore: 0.11\n",
      "i:  154\n",
      "Episode: 154  Reward: -58 Explore: 0.10\n",
      "i:  155\n",
      "Episode: 155  Reward: -51 Explore: 0.10\n",
      "i:  156\n",
      "Episode: 156  Reward: -51 Explore: 0.10\n",
      "i:  157\n",
      "Episode: 157  Reward: -51 Explore: 0.10\n",
      "i:  158\n",
      "Episode: 158  Reward: -66 Explore: 0.09\n",
      "i:  159\n",
      "Episode: 159  Reward: -58 Explore: 0.09\n",
      "i:  160\n",
      "Episode: 160  Reward: -52 Explore: 0.09\n",
      "i:  161\n",
      "Episode: 161  Reward: -51 Explore: 0.09\n",
      "i:  162\n",
      "Episode: 162  Reward: -51 Explore: 0.08\n",
      "i:  163\n",
      "Episode: 163  Reward: -59 Explore: 0.08\n",
      "i:  164\n",
      "Episode: 164  Reward: -51 Explore: 0.08\n",
      "i:  165\n",
      "Episode: 165  Reward: -59 Explore: 0.08\n",
      "i:  166\n",
      "Episode: 166  Reward: -59 Explore: 0.08\n",
      "i:  167\n",
      "Episode: 167  Reward: -53 Explore: 0.07\n",
      "i:  168\n",
      "Episode: 168  Reward: -53 Explore: 0.07\n",
      "i:  169\n",
      "Episode: 169  Reward: -56 Explore: 0.07\n",
      "i:  170\n",
      "Episode: 170  Reward: -51 Explore: 0.07\n",
      "i:  171\n",
      "Episode: 171  Reward: -63 Explore: 0.07\n",
      "i:  172\n",
      "Episode: 172  Reward: -69 Explore: 0.07\n",
      "i:  173\n",
      "Episode: 173  Reward: -52 Explore: 0.06\n",
      "i:  174\n",
      "Episode: 174  Reward: -51 Explore: 0.06\n",
      "i:  175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 175  Reward: -55 Explore: 0.06\n",
      "i:  176\n",
      "Episode: 176  Reward: -55 Explore: 0.06\n",
      "i:  177\n",
      "Episode: 177  Reward: -59 Explore: 0.06\n",
      "i:  178\n",
      "Episode: 178  Reward: -52 Explore: 0.06\n",
      "i:  179\n",
      "Episode: 179  Reward: -59 Explore: 0.05\n",
      "i:  180\n",
      "Episode: 180  Reward: -55 Explore: 0.05\n",
      "i:  181\n",
      "Episode: 181  Reward: -57 Explore: 0.05\n",
      "i:  182\n",
      "Episode: 182  Reward: -55 Explore: 0.05\n",
      "i:  183\n",
      "Episode: 183  Reward: -61 Explore: 0.05\n",
      "i:  184\n",
      "Episode: 184  Reward: -91 Explore: 0.05\n",
      "i:  185\n",
      "Episode: 185  Reward: -131 Explore: 0.05\n",
      "i:  186\n",
      "Episode: 186  Reward: -130 Explore: 0.05\n",
      "i:  187\n",
      "Episode: 187  Reward: -117 Explore: 0.04\n",
      "i:  188\n",
      "Episode: 188  Reward: -104 Explore: 0.04\n",
      "i:  189\n",
      "Episode: 189  Reward: -96 Explore: 0.04\n",
      "i:  190\n",
      "Episode: 190  Reward: -95 Explore: 0.04\n",
      "i:  191\n",
      "Episode: 191  Reward: -108 Explore: 0.04\n",
      "i:  192\n",
      "Episode: 192  Reward: -131 Explore: 0.04\n",
      "i:  193\n",
      "Episode: 193  Reward: -97 Explore: 0.04\n",
      "i:  194\n",
      "Episode: 194  Reward: -90 Explore: 0.04\n",
      "i:  195\n",
      "Episode: 195  Reward: -101 Explore: 0.04\n",
      "i:  196\n",
      "Episode: 196  Reward: -90 Explore: 0.04\n",
      "i:  197\n",
      "Episode: 197  Reward: -104 Explore: 0.04\n",
      "i:  198\n",
      "Episode: 198  Reward: -114 Explore: 0.03\n",
      "i:  199\n",
      "Episode: 199  Reward: -103 Explore: 0.03\n"
     ]
    }
   ],
   "source": [
    "for i in range(MAX_EPISODES):\n",
    "    print(\"i: \", i)\n",
    "    s = env.reset()\n",
    "    ep_reward = 0\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "        # Add exploration noise\n",
    "        a = ddpg.choose_action(s)\n",
    "        \n",
    "        # print(\"a: \", a)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        # print(\"r: \", r)\n",
    "        ddpg.store_transition(s, a, r, s_)\n",
    "\n",
    "        s = s_\n",
    "        ep_reward += r\n",
    "        \n",
    "        if j == MAX_EP_STEPS-1:\n",
    "            r_save.append(ep_reward)\n",
    "            print('Episode:', i, ' Reward: %i' % int(ep_reward), 'Explore: %.2f' % ddpg.var, )\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save(\"memory-action_dim5-norm\", ddpg.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reward = r_save[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERkxABBRERGDBBERGTEAEFEREb/HyHmqJKlgjRxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8cb61fc7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.plot([1, 2, 3, 4], [10, 20, 25, 30], color='lightblue', linewidth=3) # plot line\n",
    "ax.scatter(range(len(all_reward)), all_reward, color='green', marker='^') # plot points\n",
    "ax.set_xlim(0, 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skill Notes for Code:\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    model_path = \"./model_path/\"\n",
    "    save_path = saver.save(ddpg.sess, model_path)\n",
    "    load_path = saver.restore(ddpg.sess, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
