{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "View more on the tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import manipulator\n",
    "\n",
    "# reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "\n",
    "MAX_EP_STEPS = 1000\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.002    # learning rate for critic\n",
    "GAMMA = 0.9     # reward discount\n",
    "TAU = 0.01      # soft replacement\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "RENDER = False\n",
    "\n",
    "###############################  DDPG  ####################################\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound,):\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "        self.pointer = 0\n",
    "        self.sess = tf.Session()\n",
    "        self.var = 3.0\n",
    "        # self.a_replace_counter, self.c_replace_counter = 0, 0\n",
    "\n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,\n",
    "        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'r')\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.a = self.build_a_nn(self.S, scope='eval', trainable=True)\n",
    "            a_ = self.build_a_nn(self.S_, scope='target', trainable=False)\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # assign self.a = a in memory when calculating q for td_error,\n",
    "            # otherwise the self.a is from Actor when updating Actor\n",
    "            q = self.build_c_nn(self.S, self.a, scope='eval', trainable=True)\n",
    "            q_ = self.build_c_nn(self.S_, a_, scope='target', trainable=False)\n",
    "\n",
    "        # networks parameters\n",
    "        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        # target net replacement\n",
    "        self.soft_replace = [[tf.assign(at, (1-TAU)*at+TAU*ae), tf.assign(ct, (1-TAU)*ct+TAU*ce)]\n",
    "            for at, ae, ct, ce in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]\n",
    "\n",
    "        q_target = self.R + GAMMA * q_\n",
    "        # in the feed_dic for the td_error, the self.a should change to actions in memory\n",
    "        td_error = tf.losses.mean_squared_error(labels=(self.R + GAMMA * q_), predictions=q)\n",
    "        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, name=\"adam-ink\", var_list = self.ce_params)\n",
    "\n",
    "        a_loss = - tf.reduce_mean(q)    # maximize the q\n",
    "        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.ae_params)\n",
    "\n",
    "        tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "       \n",
    "\n",
    "    def choose_action(self, s):\n",
    "        \n",
    "        mem_mean = np.array([  0.18452059,  -0.35890538,  -1.66553736,  -0.2750107 ,\n",
    "       -17.13754082,   8.63690853,  -5.95179653,   1.80495787,\n",
    "         3.63679147,   3.20397258,   0.08991496,   0.1359086 ,\n",
    "         0.0581659 ,  -0.10008208,   0.19316533,   1.05010045,\n",
    "         0.63001388,  -0.6520322 ,  -0.37566116,   0.4660019 ,\n",
    "        -1.31206155,   2.48003078,  -2.14276052,   0.77497691,\n",
    "         2.04645395,   1.36186349], dtype=np.float32)\n",
    "        \n",
    "        mem_std = np.array([  0.10842372,   0.26727387,   1.33274329,   0.70855492,\n",
    "        10.66058445,   9.48642063,   4.19362545,   4.67959213,\n",
    "         3.23121285,   5.86314869,   0.17379682,   0.13795905,\n",
    "         0.18436897,   0.1676985 ,   0.04388477,   1.02400482,\n",
    "         0.073884  ,   0.18871291,   0.74613923,   0.63973379,\n",
    "         1.41082799,   2.31257629,   1.40827286,   0.96961248,\n",
    "         1.59099591,   1.61366057], dtype=np.float32)\n",
    "    \n",
    "    \n",
    "        s -= mem_mean[:self.s_dim]\n",
    "        s /= mem_std[:self.s_dim]\n",
    "        \n",
    "        a = self.sess.run(self.a, {self.S: s[np.newaxis, :]})[0]\n",
    "        a = np.clip(np.random.normal(a, self.var), -2, 2)\n",
    "        \n",
    "        a *= mem_std[self.s_dim: self.s_dim + self.a_dim]\n",
    "        a += mem_mean[self.s_dim: self.s_dim + self.a_dim]\n",
    "\n",
    "        return a\n",
    "\n",
    "    def learn(self):\n",
    "        # soft target replacement\n",
    "        self.sess.run(self.soft_replace)\n",
    "\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        bt = self.memory[indices, :]\n",
    "        bs = bt[:, :self.s_dim]\n",
    "        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]\n",
    "        br = bt[:, -self.s_dim - 1: -self.s_dim]\n",
    "        bs_ = bt[:, -self.s_dim:]\n",
    "\n",
    "        self.sess.run(self.atrain, {self.S: bs})\n",
    "        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "\n",
    "        trans = np.hstack((s,a,[r],s_))\n",
    "        \n",
    "        # batch normalization\n",
    "        mem_mean = np.array([  0.18452059,  -0.35890538,  -1.66553736,  -0.2750107 ,\n",
    "       -17.13754082,   8.63690853,  -5.95179653,   1.80495787,\n",
    "         3.63679147,   3.20397258,   0.08991496,   0.1359086 ,\n",
    "         0.0581659 ,  -0.10008208,   0.19316533,   1.05010045,\n",
    "         0.63001388,  -0.6520322 ,  -0.37566116,   0.4660019 ,\n",
    "        -1.31206155,   2.48003078,  -2.14276052,   0.77497691,\n",
    "         2.04645395,   1.36186349], dtype=np.float32)\n",
    "        \n",
    "        mem_std = np.array([  0.10842372,   0.26727387,   1.33274329,   0.70855492,\n",
    "        10.66058445,   9.48642063,   4.19362545,   4.67959213,\n",
    "         3.23121285,   5.86314869,   0.17379682,   0.13795905,\n",
    "         0.18436897,   0.1676985 ,   0.04388477,   1.02400482,\n",
    "         0.073884  ,   0.18871291,   0.74613923,   0.63973379,\n",
    "         1.41082799,   2.31257629,   1.40827286,   0.96961248,\n",
    "         1.59099591,   1.61366057], dtype=np.float32)\n",
    "        \n",
    "        trans -= mem_mean\n",
    "        trans /= mem_std\n",
    "        \n",
    "        \n",
    "        # print(\"trans: \", trans)\n",
    "        index = self.pointer % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = trans\n",
    "        self.pointer += 1\n",
    "\n",
    "        if self.pointer > MEMORY_CAPACITY:\n",
    "            self.var *= 0.99995\n",
    "            self.learn()\n",
    "    def build_a_nn(self, s, scope, trainable):\n",
    "        # Actor DPG\n",
    "        with tf.variable_scope(scope):\n",
    "            l1 = tf.layers.dense(s, 30, activation = tf.nn.tanh, name = 'l1', trainable = trainable)\n",
    "            a = tf.layers.dense(l1, self.a_dim, activation = tf.nn.tanh, name = 'a', trainable = trainable)     \n",
    "            return tf.multiply(a, self.a_bound, name = \"scaled_a\")  \n",
    "    def build_c_nn(self, s, a, scope, trainable):\n",
    "        # Critic Q-leaning\n",
    "        with tf.variable_scope(scope):\n",
    "            n_l1 = 30\n",
    "            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable = trainable)\n",
    "            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable = trainable)\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], trainable = trainable)\n",
    "            net = tf.nn.tanh( tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1 )\n",
    "\n",
    "            q = tf.layers.dense(net, 1, trainable = trainable)\n",
    "            return q\n",
    "\n",
    "    \n",
    "###############################  training  ####################################\n",
    "\n",
    "env = manipulator.manipulator()\n",
    "# env = env.unwrapped\n",
    "# env.seed(1)\n",
    "\n",
    "s_dim = env.state_dim\n",
    "a_dim = env.action_dim\n",
    "a_bound = 0.2\n",
    "\n",
    "ddpg = DDPG(a_dim, s_dim, a_bound)\n",
    "\n",
    "r_save = []\n",
    "\n",
    "# var = 3  # control exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "Episode: 0  Reward: -193 Explore: 3.00\n",
      "i:  1\n",
      "Episode: 1  Reward: -197 Explore: 3.00\n",
      "i:  2\n",
      "Episode: 2  Reward: -194 Explore: 3.00\n",
      "i:  3\n",
      "Episode: 3  Reward: -195 Explore: 3.00\n",
      "i:  4\n",
      "Episode: 4  Reward: -196 Explore: 3.00\n",
      "i:  5\n",
      "Episode: 5  Reward: -194 Explore: 3.00\n",
      "i:  6\n",
      "Episode: 6  Reward: -196 Explore: 3.00\n",
      "i:  7\n",
      "Episode: 7  Reward: -196 Explore: 3.00\n",
      "i:  8\n",
      "Episode: 8  Reward: -199 Explore: 3.00\n",
      "i:  9\n",
      "Episode: 9  Reward: -195 Explore: 3.00\n",
      "i:  10\n",
      "Episode: 10  Reward: -196 Explore: 2.85\n",
      "i:  11\n",
      "Episode: 11  Reward: -198 Explore: 2.71\n",
      "i:  12\n",
      "Episode: 12  Reward: -201 Explore: 2.58\n",
      "i:  13\n",
      "Episode: 13  Reward: -203 Explore: 2.46\n",
      "i:  14\n",
      "Episode: 14  Reward: -201 Explore: 2.34\n",
      "i:  15\n",
      "Episode: 15  Reward: -200 Explore: 2.22\n",
      "i:  16\n",
      "Episode: 16  Reward: -201 Explore: 2.11\n",
      "i:  17\n",
      "Episode: 17  Reward: -197 Explore: 2.01\n",
      "i:  18\n",
      "Episode: 18  Reward: -198 Explore: 1.91\n",
      "i:  19\n",
      "Episode: 19  Reward: -201 Explore: 1.82\n",
      "i:  20\n",
      "Episode: 20  Reward: -201 Explore: 1.73\n",
      "i:  21\n",
      "Episode: 21  Reward: -197 Explore: 1.65\n",
      "i:  22\n",
      "Episode: 22  Reward: -199 Explore: 1.57\n",
      "i:  23\n",
      "Episode: 23  Reward: -201 Explore: 1.49\n",
      "i:  24\n",
      "Episode: 24  Reward: -196 Explore: 1.42\n",
      "i:  25\n",
      "Episode: 25  Reward: -203 Explore: 1.35\n",
      "i:  26\n",
      "Episode: 26  Reward: -199 Explore: 1.28\n",
      "i:  27\n",
      "Episode: 27  Reward: -197 Explore: 1.22\n",
      "i:  28\n",
      "Episode: 28  Reward: -198 Explore: 1.16\n",
      "i:  29\n",
      "Episode: 29  Reward: -199 Explore: 1.10\n",
      "i:  30\n",
      "Episode: 30  Reward: -199 Explore: 1.05\n",
      "i:  31\n",
      "Episode: 31  Reward: -198 Explore: 1.00\n",
      "i:  32\n",
      "Episode: 32  Reward: -199 Explore: 0.95\n",
      "i:  33\n",
      "Episode: 33  Reward: -200 Explore: 0.90\n",
      "i:  34\n",
      "Episode: 34  Reward: -199 Explore: 0.86\n",
      "i:  35\n",
      "Episode: 35  Reward: -198 Explore: 0.82\n",
      "i:  36\n",
      "Episode: 36  Reward: -197 Explore: 0.78\n",
      "i:  37\n",
      "Episode: 37  Reward: -199 Explore: 0.74\n",
      "i:  38\n",
      "Episode: 38  Reward: -201 Explore: 0.70\n",
      "i:  39\n",
      "Episode: 39  Reward: -198 Explore: 0.67\n",
      "i:  40\n",
      "Episode: 40  Reward: -200 Explore: 0.64\n",
      "i:  41\n",
      "Episode: 41  Reward: -201 Explore: 0.61\n",
      "i:  42\n",
      "Episode: 42  Reward: -202 Explore: 0.58\n",
      "i:  43\n",
      "Episode: 43  Reward: -200 Explore: 0.55\n",
      "i:  44\n",
      "Episode: 44  Reward: -204 Explore: 0.52\n",
      "i:  45\n",
      "Episode: 45  Reward: -201 Explore: 0.50\n",
      "i:  46\n",
      "Episode: 46  Reward: -201 Explore: 0.47\n",
      "i:  47\n",
      "Episode: 47  Reward: -201 Explore: 0.45\n",
      "i:  48\n",
      "Episode: 48  Reward: -202 Explore: 0.43\n",
      "i:  49\n",
      "Episode: 49  Reward: -201 Explore: 0.41\n",
      "i:  50\n",
      "Episode: 50  Reward: -201 Explore: 0.39\n",
      "i:  51\n",
      "Episode: 51  Reward: -202 Explore: 0.37\n",
      "i:  52\n",
      "Episode: 52  Reward: -202 Explore: 0.35\n",
      "i:  53\n",
      "Episode: 53  Reward: -202 Explore: 0.33\n",
      "i:  54\n",
      "Episode: 54  Reward: -201 Explore: 0.32\n",
      "i:  55\n",
      "Episode: 55  Reward: -202 Explore: 0.30\n",
      "i:  56\n",
      "Episode: 56  Reward: -202 Explore: 0.29\n",
      "i:  57\n",
      "Episode: 57  Reward: -203 Explore: 0.27\n",
      "i:  58\n",
      "Episode: 58  Reward: -201 Explore: 0.26\n",
      "i:  59\n",
      "Episode: 59  Reward: -201 Explore: 0.25\n",
      "i:  60\n",
      "Episode: 60  Reward: -201 Explore: 0.23\n",
      "i:  61\n",
      "Episode: 61  Reward: -202 Explore: 0.22\n",
      "i:  62\n",
      "Episode: 62  Reward: -202 Explore: 0.21\n",
      "i:  63\n",
      "Episode: 63  Reward: -202 Explore: 0.20\n",
      "i:  64\n",
      "Episode: 64  Reward: -202 Explore: 0.19\n",
      "i:  65\n",
      "Episode: 65  Reward: -202 Explore: 0.18\n",
      "i:  66\n",
      "Episode: 66  Reward: -202 Explore: 0.17\n",
      "i:  67\n",
      "Episode: 67  Reward: -202 Explore: 0.17\n",
      "i:  68\n",
      "Episode: 68  Reward: -202 Explore: 0.16\n",
      "i:  69\n",
      "Episode: 69  Reward: -202 Explore: 0.15\n",
      "i:  70\n",
      "Episode: 70  Reward: -202 Explore: 0.14\n",
      "i:  71\n",
      "Episode: 71  Reward: -202 Explore: 0.14\n",
      "i:  72\n",
      "Episode: 72  Reward: -201 Explore: 0.13\n",
      "i:  73\n",
      "Episode: 73  Reward: -201 Explore: 0.12\n",
      "i:  74\n",
      "Episode: 74  Reward: -202 Explore: 0.12\n",
      "i:  75\n",
      "Episode: 75  Reward: -202 Explore: 0.11\n",
      "i:  76\n",
      "Episode: 76  Reward: -202 Explore: 0.11\n",
      "i:  77\n",
      "Episode: 77  Reward: -202 Explore: 0.10\n",
      "i:  78\n",
      "Episode: 78  Reward: -202 Explore: 0.10\n",
      "i:  79\n",
      "Episode: 79  Reward: -202 Explore: 0.09\n",
      "i:  80\n",
      "Episode: 80  Reward: -202 Explore: 0.09\n",
      "i:  81\n",
      "Episode: 81  Reward: -202 Explore: 0.08\n",
      "i:  82\n",
      "Episode: 82  Reward: -202 Explore: 0.08\n",
      "i:  83\n",
      "Episode: 83  Reward: -202 Explore: 0.07\n",
      "i:  84\n",
      "Episode: 84  Reward: -202 Explore: 0.07\n",
      "i:  85\n",
      "Episode: 85  Reward: -202 Explore: 0.07\n",
      "i:  86\n",
      "Episode: 86  Reward: -202 Explore: 0.06\n",
      "i:  87\n",
      "Episode: 87  Reward: -202 Explore: 0.06\n",
      "i:  88\n",
      "Episode: 88  Reward: -202 Explore: 0.06\n",
      "i:  89\n",
      "Episode: 89  Reward: -202 Explore: 0.05\n",
      "i:  90\n",
      "Episode: 90  Reward: -202 Explore: 0.05\n",
      "i:  91\n",
      "Episode: 91  Reward: -201 Explore: 0.05\n",
      "i:  92\n",
      "Episode: 92  Reward: -201 Explore: 0.05\n",
      "i:  93\n",
      "Episode: 93  Reward: -201 Explore: 0.04\n",
      "i:  94\n",
      "Episode: 94  Reward: -201 Explore: 0.04\n",
      "i:  95\n",
      "Episode: 95  Reward: -201 Explore: 0.04\n",
      "i:  96\n",
      "Episode: 96  Reward: -201 Explore: 0.04\n",
      "i:  97\n",
      "Episode: 97  Reward: -201 Explore: 0.04\n",
      "i:  98\n",
      "Episode: 98  Reward: -201 Explore: 0.04\n",
      "i:  99\n",
      "Episode: 99  Reward: -201 Explore: 0.03\n",
      "i:  100\n",
      "Episode: 100  Reward: -201 Explore: 0.03\n",
      "i:  101\n",
      "Episode: 101  Reward: -201 Explore: 0.03\n",
      "i:  102\n",
      "Episode: 102  Reward: -202 Explore: 0.03\n",
      "i:  103\n",
      "Episode: 103  Reward: -202 Explore: 0.03\n",
      "i:  104\n",
      "Episode: 104  Reward: -202 Explore: 0.03\n",
      "i:  105\n",
      "Episode: 105  Reward: -202 Explore: 0.02\n",
      "i:  106\n",
      "Episode: 106  Reward: -201 Explore: 0.02\n",
      "i:  107\n",
      "Episode: 107  Reward: -201 Explore: 0.02\n",
      "i:  108\n",
      "Episode: 108  Reward: -201 Explore: 0.02\n",
      "i:  109\n",
      "Episode: 109  Reward: -201 Explore: 0.02\n",
      "i:  110\n",
      "Episode: 110  Reward: -201 Explore: 0.02\n",
      "i:  111\n",
      "Episode: 111  Reward: -201 Explore: 0.02\n",
      "i:  112\n",
      "Episode: 112  Reward: -201 Explore: 0.02\n",
      "i:  113\n",
      "Episode: 113  Reward: -201 Explore: 0.02\n",
      "i:  114\n",
      "Episode: 114  Reward: -201 Explore: 0.02\n",
      "i:  115\n",
      "Episode: 115  Reward: -201 Explore: 0.01\n",
      "i:  116\n",
      "Episode: 116  Reward: -201 Explore: 0.01\n",
      "i:  117\n",
      "Episode: 117  Reward: -201 Explore: 0.01\n",
      "i:  118\n",
      "Episode: 118  Reward: -201 Explore: 0.01\n",
      "i:  119\n",
      "Episode: 119  Reward: -201 Explore: 0.01\n",
      "i:  120\n",
      "Episode: 120  Reward: -201 Explore: 0.01\n",
      "i:  121\n",
      "Episode: 121  Reward: -201 Explore: 0.01\n",
      "i:  122\n",
      "Episode: 122  Reward: -201 Explore: 0.01\n",
      "i:  123\n",
      "Episode: 123  Reward: -201 Explore: 0.01\n",
      "i:  124\n",
      "Episode: 124  Reward: -202 Explore: 0.01\n",
      "i:  125\n",
      "Episode: 125  Reward: -202 Explore: 0.01\n",
      "i:  126\n",
      "Episode: 126  Reward: -202 Explore: 0.01\n",
      "i:  127\n",
      "Episode: 127  Reward: -202 Explore: 0.01\n",
      "i:  128\n",
      "Episode: 128  Reward: -202 Explore: 0.01\n",
      "i:  129\n",
      "Episode: 129  Reward: -202 Explore: 0.01\n",
      "i:  130\n",
      "Episode: 130  Reward: -202 Explore: 0.01\n",
      "i:  131\n",
      "Episode: 131  Reward: -202 Explore: 0.01\n",
      "i:  132\n",
      "Episode: 132  Reward: -202 Explore: 0.01\n",
      "i:  133\n",
      "Episode: 133  Reward: -202 Explore: 0.01\n",
      "i:  134\n",
      "Episode: 134  Reward: -202 Explore: 0.01\n",
      "i:  135\n",
      "Episode: 135  Reward: -202 Explore: 0.01\n",
      "i:  136\n",
      "Episode: 136  Reward: -202 Explore: 0.01\n",
      "i:  137\n",
      "Episode: 137  Reward: -202 Explore: 0.00\n",
      "i:  138\n",
      "Episode: 138  Reward: -202 Explore: 0.00\n",
      "i:  139\n",
      "Episode: 139  Reward: -202 Explore: 0.00\n",
      "i:  140\n",
      "Episode: 140  Reward: -202 Explore: 0.00\n",
      "i:  141\n",
      "Episode: 141  Reward: -202 Explore: 0.00\n",
      "i:  142\n",
      "Episode: 142  Reward: -202 Explore: 0.00\n",
      "i:  143\n",
      "Episode: 143  Reward: -202 Explore: 0.00\n",
      "i:  144\n",
      "Episode: 144  Reward: -202 Explore: 0.00\n",
      "i:  145\n",
      "Episode: 145  Reward: -202 Explore: 0.00\n",
      "i:  146\n",
      "Episode: 146  Reward: -202 Explore: 0.00\n",
      "i:  147\n",
      "Episode: 147  Reward: -202 Explore: 0.00\n",
      "i:  148\n",
      "Episode: 148  Reward: -202 Explore: 0.00\n",
      "i:  149\n",
      "Episode: 149  Reward: -202 Explore: 0.00\n",
      "i:  150\n",
      "Episode: 150  Reward: -202 Explore: 0.00\n",
      "i:  151\n",
      "Episode: 151  Reward: -202 Explore: 0.00\n",
      "i:  152\n",
      "Episode: 152  Reward: -202 Explore: 0.00\n",
      "i:  153\n",
      "Episode: 153  Reward: -202 Explore: 0.00\n",
      "i:  154\n",
      "Episode: 154  Reward: -202 Explore: 0.00\n",
      "i:  155\n",
      "Episode: 155  Reward: -202 Explore: 0.00\n",
      "i:  156\n",
      "Episode: 156  Reward: -202 Explore: 0.00\n",
      "i:  157\n",
      "Episode: 157  Reward: -202 Explore: 0.00\n",
      "i:  158\n",
      "Episode: 158  Reward: -202 Explore: 0.00\n",
      "i:  159\n",
      "Episode: 159  Reward: -202 Explore: 0.00\n",
      "i:  160\n",
      "Episode: 160  Reward: -202 Explore: 0.00\n",
      "i:  161\n",
      "Episode: 161  Reward: -202 Explore: 0.00\n",
      "i:  162\n",
      "Episode: 162  Reward: -202 Explore: 0.00\n",
      "i:  163\n",
      "Episode: 163  Reward: -202 Explore: 0.00\n",
      "i:  164\n",
      "Episode: 164  Reward: -202 Explore: 0.00\n",
      "i:  165\n",
      "Episode: 165  Reward: -202 Explore: 0.00\n",
      "i:  166\n",
      "Episode: 166  Reward: -202 Explore: 0.00\n",
      "i:  167\n",
      "Episode: 167  Reward: -202 Explore: 0.00\n",
      "i:  168\n",
      "Episode: 168  Reward: -202 Explore: 0.00\n",
      "i:  169\n",
      "Episode: 169  Reward: -202 Explore: 0.00\n",
      "i:  170\n",
      "Episode: 170  Reward: -202 Explore: 0.00\n",
      "i:  171\n",
      "Episode: 171  Reward: -202 Explore: 0.00\n",
      "i:  172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 172  Reward: -202 Explore: 0.00\n",
      "i:  173\n",
      "Episode: 173  Reward: -202 Explore: 0.00\n",
      "i:  174\n",
      "Episode: 174  Reward: -202 Explore: 0.00\n",
      "i:  175\n",
      "Episode: 175  Reward: -202 Explore: 0.00\n",
      "i:  176\n",
      "Episode: 176  Reward: -202 Explore: 0.00\n",
      "i:  177\n",
      "Episode: 177  Reward: -202 Explore: 0.00\n",
      "i:  178\n",
      "Episode: 178  Reward: -202 Explore: 0.00\n",
      "i:  179\n",
      "Episode: 179  Reward: -202 Explore: 0.00\n",
      "i:  180\n",
      "Episode: 180  Reward: -202 Explore: 0.00\n",
      "i:  181\n",
      "Episode: 181  Reward: -202 Explore: 0.00\n",
      "i:  182\n",
      "Episode: 182  Reward: -202 Explore: 0.00\n",
      "i:  183\n",
      "Episode: 183  Reward: -202 Explore: 0.00\n",
      "i:  184\n",
      "Episode: 184  Reward: -202 Explore: 0.00\n",
      "i:  185\n",
      "Episode: 185  Reward: -202 Explore: 0.00\n",
      "i:  186\n",
      "Episode: 186  Reward: -202 Explore: 0.00\n",
      "i:  187\n",
      "Episode: 187  Reward: -202 Explore: 0.00\n",
      "i:  188\n",
      "Episode: 188  Reward: -202 Explore: 0.00\n",
      "i:  189\n",
      "Episode: 189  Reward: -202 Explore: 0.00\n",
      "i:  190\n",
      "Episode: 190  Reward: -202 Explore: 0.00\n",
      "i:  191\n",
      "Episode: 191  Reward: -202 Explore: 0.00\n",
      "i:  192\n",
      "Episode: 192  Reward: -202 Explore: 0.00\n",
      "i:  193\n",
      "Episode: 193  Reward: -202 Explore: 0.00\n",
      "i:  194\n",
      "Episode: 194  Reward: -202 Explore: 0.00\n",
      "i:  195\n",
      "Episode: 195  Reward: -202 Explore: 0.00\n",
      "i:  196\n",
      "Episode: 196  Reward: -202 Explore: 0.00\n",
      "i:  197\n",
      "Episode: 197  Reward: -202 Explore: 0.00\n",
      "i:  198\n",
      "Episode: 198  Reward: -202 Explore: 0.00\n",
      "i:  199\n",
      "Episode: 199  Reward: -202 Explore: 0.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(MAX_EPISODES):\n",
    "    print(\"i: \", i)\n",
    "    s = env.reset()\n",
    "    ep_reward = 0\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "        # Add exploration noise\n",
    "        a = ddpg.choose_action(s)\n",
    "        # a = np.clip(np.random.normal(a, var), -2, 2)    # add randomness to action selection for exploration\n",
    "        # print(\"a: \", a)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        # print(\"r: \", r)\n",
    "        ddpg.store_transition(s, a, r, s_)\n",
    "\n",
    "        s = s_\n",
    "        ep_reward += r\n",
    "\n",
    "        \n",
    "\n",
    "        if j == MAX_EP_STEPS-1:\n",
    "            r_save.append(ep_reward)\n",
    "            print('Episode:', i, ' Reward: %i' % int(ep_reward), 'Explore: %.2f' % ddpg.var, )\n",
    "            # if ep_reward > -300:RENDER = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reward = r_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIgICgYiIoKCgYiIoGAgIiLA/wexwXDt8E0LUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f70e4f82748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.plot([1, 2, 3, 4], [10, 20, 25, 30], color='lightblue', linewidth=3) # plot line\n",
    "ax.scatter(range(len(all_reward)), all_reward, color='green', marker='^') # plot points\n",
    "# ax.set_xlim(0.5, 4.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"memory-action_dim5-steps1000\", ddpg.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max-step-1000.png\n",
    "\n",
    "<img src=\"img/max-step-1000.png\">\n",
    "\n",
    "Analysis: \n",
    "### 1st\n",
    ">Episode: 9  Reward: -195 Explore: 3.00\n",
    "i:  10\n",
    "Episode: 10  Reward: -196 Explore: 2.85\n",
    "\n",
    ">Training Start from Episode 10.\n",
    "\n",
    "The MEMORY_CAPACITY should be bigger.\n",
    "\n",
    "### 2nd\n",
    ">Episode: 89  Reward: -202 Explore: 0.05\n",
    "\n",
    "The decrease of var should be more slowly.\n",
    "\n",
    "### 3rd\n",
    "The Difference Values of Mean and Std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
